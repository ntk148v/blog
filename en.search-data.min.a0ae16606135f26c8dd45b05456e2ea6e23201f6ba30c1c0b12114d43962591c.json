[{"id":0,"href":"/blog/posts/docker-iptables/","title":"Docker and Iptables: You may do it wrong!","section":"./posts/","content":" 1. Mission # If you\u0026rsquo;re running Docker on a host that is exposed to the Internet (network bridge), you will probably want to restrict external access.\n2. Docker network # Let\u0026rsquo;s start with a fact that Docker manipulates iptables rules to provide network isolation, on Linux. Docker installs custom iptables chains named DOCKER, DOCKER-USER and DOCKER-ISOLATION-STAGE-*, and it ensures that incoming packets are always checked by these chains first.\niptables -L -n Re.Docker network, I won\u0026rsquo;t describe here because it\u0026rsquo;s a lot of knowledge. You may want to check Docker network note. I will only show iptables packet flow: For example, we have a host with ip 10.0.10.26, then start a container that is exposed port 8000 to internet.\nThis is a basic packet flow from outside:\n3. You may do it wrong - common mistakes # Alright, before we make it right, there are some common mistakes.\n3.1. Modify Docker generated rules manually # Docker generates iptables rules, then adds to DOCKER chains. Some users may manipulate this chain manually in order to block connections.\nPlease don\u0026rsquo;t do it. Yes, you are able to do it, there is nothing prevent you to perform this kind of action. But every time Docker daemon is reloaded, iptables rules are re-generated and your changes is gone.\nRight: Do not manipulate Docker rules manually. 3.2. Insert you rules in the wrong chain # iptables basic: iptables is divied into three levels: tables, chains and rules. We only use the filter tables, which contains:\nINPUT: Packets sent to this host pass through this chain. OUTPUT: Packets sent from this host pass through this chain. FORWARD: Packets forwarded by this host pass through this chain. Commonly, to block connection from external, put reject rules in INPUT chain. But in Docker, it doesn\u0026rsquo;t work. Look at the above packet flow, the packet doesn\u0026rsquo;t pass through INPUT chain, it goes in FORWARD chain. Since Docker connects the bridge (default docker0) to the default gateway (external interface ens33 for example) via Network Address Translation (NAT) by default, setting INPUT is useless and the FORWARD chain should be set. And since the FORWARD chain defaults to DROP, all forwarding is blocked by DOCKER-USER.\nRight: Add rules which load before Docker\u0026rsquo;s rules, add them to DOCKER-USER. 3.3. Modify and persistent iptables wrong # You modify and persistent iptables rules like this:\nSave all rules iptables-save. Modify your rules. Restore all rules with iptables-restore. Persistent and load on start up using iptables-services or rc.local. This implementation has a drawback, every time Docker container changes, you need to save the current iptables configuration, otherwise when executing iptables-restore, it will load the old rules, which will lead to confusing iptables rules.\nRight: Do not save, flush then restore all rules. Check the following solution. 4. Do it right! # 4.1. Overview # I have create a repository for this, which is highly inspired by systemd-service-iptables: https://github.com/ntk148v/systemd-iptables\nWhitelist strategy: block all, allow some.\niptables-restore -n|--no-flush turns off implicit global refresh and only performs our manual explicit refresh: only modified chains are flushed.\nControl iptables with systemd: start iptables after other services.\niptables.service. [Unit] Documentation=man:iptables man:iptables-restore [Service] Type=oneshot ExecStart=/bin/true RemainAfterExit=on [Install] WantedBy=multi-user.target iptables@.service. [Unit] DefaultDependencies=no Wants=network-pre.target Wants=systemd-modules-load.service Wants=local-fs.target Before=network-pre.target Before=shutdown.target After=systemd-modules-load.service After=local-fs.target After=dbus.service After=polkit.service PartOf=iptables.service Conflicts=shutdown.target ConditionPathExists=/etc/iptables/ Documentation=man:iptables man:iptables-restore [Service] Environment=IPTABLES_RULES=/etc/iptables/%i.rules Environment=IPTABLES_RULES_FLUSH=/etc/iptables/%i.rules.empty Type=oneshot # Start iptables-restore with no-flush option with %i.rules # NoFlush option (-n) is used to not flush other tables than %I. ExecStart=/sbin/iptables-restore -v -n $IPTABLES_RULES # Reload the rules ExecReload=/sbin/iptables-restore -v -n $IPTABLES_RULES # Stop and clean with empty rules ExecStop=/sbin/iptables-restore -v -n $IPTABLES_RULES_FLUSH RemainAfterExit=yes [Install] WantedBy=multi-user.target # Need /etc/iptables/base.rules to exist. DefaultInstance=base Templating so nobody can\u0026rsquo;t go wrong:\nbase.rules.empty. *filter # Reset counters :INPUT ACCEPT [0:0] :FORWARD DROP [0:0] :OUTPUT ACCEPT [0:0] :DOCKER-USER - [0:0] # Flush -F INPUT -F OUTPUT -F DOCKER-USER COMMIT base.rules. *filter # Reset counters :INPUT ACCEPT [0:0] :FORWARD DROP [0:0] :OUTPUT ACCEPT [0:0] :DOCKER-USER - [0:0] # Flush -F INPUT -F OUTPUT -F DOCKER-USER ######### # INPUT # ######### # Accept on localhost -A INPUT -i lo -m comment --comment \u0026#34;Default: Accept loopback\u0026#34; -j ACCEPT # Accept on Docker bridge -A INPUT -i br+ -m comment --comment \u0026#34;Default: Accept bridge networks\u0026#34; -j ACCEPT -A INPUT -i docker0 -m comment --comment \u0026#34;Default: Accept docker0\u0026#34; -j ACCEPT # Allow established sessions to receive traffic -A INPUT -m state --state RELATED,ESTABLISHED -m comment --comment \u0026#34;Default: Accept RELATED, ESTABLISHED connections\u0026#34; -j ACCEPT -A INPUT -p icmp -m comment --comment \u0026#34;Default: Accept ICMP\u0026#34; -j ACCEPT # Insert your INPUT ACCEPT rules here # For example: # Open https port only for 1 ip: # -A INPUT -s 10.1.1.1/32 -p tcp -m tcp --dport 443 -j ACCEPT # By default reject all packets and leave a logging -A INPUT -m comment --comment \u0026#34;Default: Log INPUT dropped packets\u0026#34; -j LOG --log-prefix \u0026#34;iptables INPUT DROP \u0026#34; --log-level 7 -A INPUT -j REJECT --reject-with icmp-host-prohibited ########## # OUTPUT # ########## # Accept on localhost -A OUTPUT -o lo -j ACCEPT # Accept Docker networks -A OUTPUT -o br+ -m comment --comment \u0026#34;Default: Accept bridge networks\u0026#34; -j ACCEPT -A OUTPUT -o docker0 -m comment --comment \u0026#34;Default: Accept docker0\u0026#34; -j ACCEPT # Allow established sessions to receive traffic -A OUTPUT -m state --state RELATED,ESTABLISHED -m comment --comment \u0026#34;Default: Accept RELATED, ESTABLISHED connections\u0026#34; -j ACCEPT # Insert your OUTPUT ACCEPT rules here # By default reject all packets and leave a logging -A OUTPUT -m comment --comment \u0026#34;Default: Log OUTPUT dropped packets\u0026#34; -j LOG --log-prefix \u0026#34;iptables OUTPUT DROP \u0026#34; --log-level 7 -A OUTPUT -j REJECT --reject-with icmp-host-prohibited ############### # DOCKER-USER # ############### # Change your external interface here! # Allow established sessions to receive traffic -A DOCKER-USER -i extinf -m state --state established,related -m comment --comment \u0026#34;Default: Accept RELATED, ESTABLISHED connections\u0026#34; -j ACCEPT -A DOCKER-USER -i extinf -m conntrack --ctstate RELATED,ESTABLISHED -m comment --comment \u0026#34;Default: Accept RELATED, ESTABLISHED connections\u0026#34; -j ACCEPT -A DOCKER-USER -o extinf -m state --state established,related -m comment --comment \u0026#34;Default: Accept RELATED, ESTABLISHED connections\u0026#34; -j ACCEPT -A DOCKER-USER -o extinf -m conntrack --ctstate RELATED,ESTABLISHED -m comment --comment \u0026#34;Default: Accept RELATED, ESTABLISHED connections\u0026#34; -j ACCEPT # Insert your DOCKER-USER ACCEPT rules here # Note that you have to use conntrack and ctorigdstport due to NAT. # For example: # Allow all on http # -A DOCKER-USER -i ens192 -p tcp -m tcp -m conntrack --ctorigdstport 80 -j ACCEPT # By default reject all packets -A DOCKER-USER -m comment --comment \u0026#34;Default: Log DROP-USER input dropped packets in external inteface\u0026#34; -j LOG --log-prefix \u0026#34;iptables DOCKER-USER INPUT DROP in external interface\u0026#34; --log-level 7 -A DOCKER-USER -i extinf -j REJECT -A DOCKER-USER -m comment --comment \u0026#34;Default: Log DROP-USER output dropped packets in external inteface\u0026#34; -j LOG --log-prefix \u0026#34;iptables DOCKER-USER OUTPUT DROP in external interface\u0026#34; --log-level 7 -A DOCKER-USER -o extinf -j REJECT -A DOCKER-USER -j RETURN ## Commit COMMIT 4.2. Getting started # Ofc you need iptables and systemd installed. On the Linux, run as root: git clone https://github.com/ntk148v/systemd-iptables cd systemd-iptables # Edit the rules in etc/iptables/base.rules as needed. # and install the service cp -Rv etc/. /etc/ Make changes in /etc/iptables/base.rules. Replace the placeholder extinf interface in the rulebook with your actual external interface (eth0 for e.x). Add your custom allow rules in the right place! Allow inbound connections -\u0026gt; INPUT Custom Accept Block. Allow outbound connections -\u0026gt; OUTPUT Custom Accept Block. Allow inbound/outbound connections to your Docker containers using network bridge -\u0026gt; DOCKER-USER Custom Accept Block. After that, enable the serivces and we are done: systemctl daemon-reload systemctl enable iptables.service systemctl enable iptables@base.service systemctl start iptables@base.service # Check status systemctl status iptables@base.service If you make any changes in the future, make sure to restart/reload your service. systemctl restart iptables@base.service 5. References # https://docs.docker.com/network/iptables/ https://github.com/boTux-fr/systemd-service-iptables "},{"id":1,"href":"/blog/posts/moving-from-utterances-to-giscus/","title":"Moving from Utterances to Giscus","section":"./posts/","content":"I\u0026rsquo;ve used Utterances for a while. Using Github\u0026rsquo;s issue feature as a backend for comments is a very elegant solution IM O: no tracking, no ads, simple. But today, I decide to switch to a better alternative - Giscus. Giscus is heavily inspired by Utterances except one thing: instead of using Github issues it uses the fairly new Discussions features to store comments.\nSo..\nWhy migrate? # Post reactions: utterances allows you to add reactions to comments but as an author I’m also interested in the general reception of the post itself. giscus provides this feature. Conversation view: utterances will simply render comments as a list in the order they have been created. Giscus groups replies to a comment instead. I mean comment is about discussion, right? Prepare your site # Migrate from utterances: convert the existing issues into discussions. Follow Giscus, it\u0026rsquo;s quite simple. Leave a comment 👇\n"},{"id":2,"href":"/blog/posts/bgp-ecmp-load-balancing/","title":"Bgp Ecmp Load Balancing","section":"./posts/","content":" 1. Introduction # We will build a Load balancer with BGP and Equal-Cost Multipath routing (ECMP) using both Bird and ExaBGP.\nReferences:\nHow to build a load balancer with BGP and ECMP using VyOS Multi-tier load balancer Load balancing without Load balancers 2. Lab overview # EVE-NG version 2.0.3-112 QEMU version 2.4.0 AS 65000: internet service provider. In this post, we will build a BGP session between EdgeRouter and ISP router. ISPRouter and EdgeRouter are Fortinet Fortigate v7.0.3 instances. You can use other routers as well. Switch is a Cisco switch. client, lb1, and lb2 are Ubuntu server 18.04 instances. lb1 and lb2 will be in the 10.12.12.0/24 private LAN, we will install nginx (LB L7) on these. Both servers will announce the same public IP (10.13.13.1) to EdgeRouter using BGP. Incoming traffic from internet to this public IP will be routed to lb1 or lb2 depending of a hash. You need to download and install device virtual images. Follow EVE-NG guide. 3. Configure # 3.1. ISPRouter # Follow the Fortigate document for the basic commands.\n# interfaces configurations config system interface edit \u0026#34;port1\u0026#34; set mode static set ip 192.168.22.226 255.255.255.0 set allowaccess ping https ssh http telnet next edit \u0026#34;port2\u0026#34; set mode static set ip 10.0.0.254 255.255.255.0 set allowaccess ping https ssh http telnet next edit \u0026#34;port3\u0026#34; set mode static set ip 172.16.42.2 255.255.255.254 set allowaccess ping https ssh http telnet next end # Simple BGP configuration config router bgp set as 65000 set router-id 172.16.42.2 config neighbor edit \u0026#34;172.16.42.3\u0026#34; set capability-default-originate enable set remote-as 65500 set update-source \u0026#34;port3\u0026#34; next end end # Firewall policy (simple allow all) config firewall policy edit 1 set name \u0026#34;allow\u0026#34; set srcintf \u0026#34;any\u0026#34; set dstintf \u0026#34;any\u0026#34; set action accept set srcaddr \u0026#34;all\u0026#34; set dstaddr \u0026#34;all\u0026#34; set schedule \u0026#34;always\u0026#34; set service \u0026#34;ALL\u0026#34; next end 3.2. EdgeRouter # config system interface edit \u0026#34;port1\u0026#34; set vdom \u0026#34;root\u0026#34; set ip 172.16.42.3 255.255.255.254 set allowaccess ping https ssh snmp http telnet set type physical set snmp-index 1 next edit \u0026#34;port2\u0026#34; set vdom \u0026#34;root\u0026#34; set ip 10.12.12.254 255.255.255.0 set allowaccess ping https ssh snmp http telnet set type physical set snmp-index 2 next end # BGP config config router bgp set as 65500 set router-id 172.16.42.3 set ibgp-multipath enable set additional-path enable config neighbor edit \u0026#34;10.12.12.2\u0026#34; set remote-as 65500 set update-source \u0026#34;port2\u0026#34; next edit \u0026#34;10.12.12.1\u0026#34; set remote-as 65500 set update-source \u0026#34;port2\u0026#34; next edit \u0026#34;172.16.42.2\u0026#34; set remote-as 65000 set update-source \u0026#34;port1\u0026#34; next end end # Firewall policy (simple allow all) config firewall policy edit 1 set name \u0026#34;allow\u0026#34; set srcintf \u0026#34;any\u0026#34; set dstintf \u0026#34;any\u0026#34; set action accept set srcaddr \u0026#34;all\u0026#34; set dstaddr \u0026#34;all\u0026#34; set schedule \u0026#34;always\u0026#34; set service \u0026#34;ALL\u0026#34; next end You can change load-balancing algorithms. By default, it is source-ip-based. config system settings set v4-ecmp-mode {source-ip-based* | weight-based | usage-based | source-dest-ip-based} end 3.3. Switch # Set up mode access. configure terminal interface range Ethernet 0/0-2 switchport mode access switchport access vlan 2 exit copy running-config start-config 3.4. Servers # Configure 10.13.13.1 on the local loopback interface. lb1$ sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/netplan/00-installer-config.yaml network: ethernets: lo: addresses: - 10.13.13.1/24 ens3: addresses: - 10.12.12.1/24 # change to 10.12.12.2 on lb2 gateway4: 10.12.12.254 version: 2 EOF lb1$ sudo netplan apply Install nginx. lb1$ sudo apt install nginx -y Configure nginx: lb1$ sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/nginx/sites-enabled/default server { listen 80 default_server; listen [::]:80 default_server; root /var/www/html; } EOF # In lb1 lb1$ echo lb1 \u0026gt; /var/www/html/hostname # In lb2 lb2$ echo lb2 \u0026gt; /var/www/html/hostname lb1$ sudo service nginx start Choose one of the follow configurations (Bird or ExaBGP).\n3.4.1. Configure Bird # We will use Bird. Install bird lb1$ sudo apt install bird -y Configure bird. lb1$ sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/bird.conf protocol kernel { persist; scan time 20; export all; } protocol device { scan time 10; } protocol static { } protocol static static_bgp { import all; route 10.13.13.1/32 reject; } protocol bgp { local as 65500; neighbor 10.12.12.254 as 65500; import none; export where proto = \u0026#34;static_bgp\u0026#34;; } EOF Start bird. lb1$ sudo service bird start lb1$ sudo service bird status _ bird.service - BIRD Internet Routing Daemon (IPv4) Loaded: loaded (/lib/systemd/system/bird.service; disabled; vendor preset: enabled) Active: inactive (dead) Feb 07 08:11:13 lb2 systemd[1]: Starting BIRD Internet Routing Daemon (IPv4)... Feb 07 08:11:13 lb2 systemd[1]: Started BIRD Internet Routing Daemon (IPv4). Feb 07 08:11:13 lb2 bird[884]: Chosen router ID 10.12.12.2 according to interface ens3 Feb 07 08:11:13 lb2 bird[884]: Started 3.4.2. Configure ExaBGP # Instead of Bird, we can also use ExaBGP. ExaBGP provides a convenient way to implement Software Defined Networking by transforming BGP messages into friendly plain text or JSON, which can then be easily handled by simple scripts or your BSS/OSS. Install ExaBGP lb1$ sudo apt install python3-pip python3-dev -y lb1$ sudo pip3 install exabgp lb1$ exabgp --run healthcheck --help Create exabgp user and group. lb1$ sudo useradd exabgp Configure ExaBGP lb1$ sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/exabgp/exabgp.conf neighbor 10.12.12.254 { # Remote neighbor to peer with router-id 10.12.12.1; # Local router-id, change to 10.12.12.2 on lb2 local-address 10.12.12.1; # Local update-router, change to 10.12.12.2 on lb2 local-as 65500; # Local AS peer-as 65500; # Peer AS family { ipv4 unicast; } } process watch-nginx { run python3 -m exabgp healthcheck --cmd \u0026#34;curl -sf http://10.13.13.1\u0026#34; --label nginx --ip 10.13.13.1/32; encoder json; } EOF Configure ExaBGP service lb1$ sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /lib/systemd/system/exabgp.service [Unit] Description=ExaBGP Documentation=man:exabgp(1) Documentation=man:exabgp.conf(5) Documentation=https://github.com/Exa-Networks/exabgp/wiki After=network.target ConditionPathExists=/etc/exabgp/exabgp.conf [Service] User=exabgp Group=exabgp Environment=exabgp_daemon_daemonize=false RuntimeDirectory=exabgp RuntimeDirectoryMode=0750 ExecStart=/usr/local/bin/exabgp /etc/exabgp/exabgp.conf ExecReload=/bin/kill -USR1 $MAINPID Restart=always CapabilityBoundingSet=CAP_NET_ADMIN AmbientCapabilities=CAP_NET_ADMIN [Install] WantedBy=multi-user.target EOF lb1$ sudo service exabgp start lb1$ sudo service exabgp status 4. Validate # To make sure everything works as expected.\n4.1. Scenario 1: Both servers are OK # Client check. client$ curl http://10.13.13.1/hostname lb1 Change client ip. The load balancing works!\n# Set client\u0026#39;s ip to 10.0.0.1 client$ ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:50:00:00:05:00 brd ff:ff:ff:ff:ff:ff inet 10.0.0.1/24 brd 10.0.0.255 scope global ens3 valid_lft forever preferred_lft forever inet6 fe80::250:ff:fe00:500/64 scope link valid_lft forever preferred_lft forever client$ curl http://10.13.13.1/hostname lb1 Change client\u0026rsquo;s ip address to 10.0.0.100 and send another request. You can the request was sent to lb2 instead lb1. client$ curl http://10.13.13.1/hostname lb2 Check EdgeRouter. FortiGate-VM64-KVM # get router info routing-table all Codes: K - kernel, C - connected, S - static, R - RIP, B - BGP O - OSPF, IA - OSPF inter area N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2 E1 - OSPF external type 1, E2 - OSPF external type 2 i - IS-IS, L1 - IS-IS level-1, L2 - IS-IS level-2, ia - IS-IS inter area * - candidate default Routing table for VRF=0 B* 0.0.0.0/0 [20/0] via 172.16.42.2 (recursive is directly connected, port1), 01:01:52 C 10.12.12.0/24 is directly connected, port2 B 10.13.13.1/32 [200/100] via 10.12.12.1 (recursive is directly connected, port2), 01:04:33 [200/100] via 10.12.12.2 (recursive is directly connected, port2), 01:04:33 C 172.16.42.2/31 is directly connected, port1 FortiGate-VM64-KVM # get router info bgp network VRF 0 BGP table version is 3, local router ID is 172.16.42.3 Status codes: s suppressed, d damped, h history, * valid, \u0026gt; best, i - internal, S Stale Origin codes: i - IGP, e - EGP, ? - incomplete Network Next Hop Metric LocPrf Weight RouteTag Path *\u0026gt; 0.0.0.0/0 172.16.42.2 0 0 0 65000 i \u0026lt;-/1\u0026gt; *\u0026gt;i10.13.13.1/32 10.12.12.2 100 100 0 0 i \u0026lt;-/2\u0026gt; *\u0026gt;i 10.12.12.1 100 100 0 0 i \u0026lt;-/1\u0026gt; Total number of prefixes 2 Check ISPRouter FortiGate-VM64-KVM # get router info routing-table all Codes: K - kernel, C - connected, S - static, R - RIP, B - BGP O - OSPF, IA - OSPF inter area N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2 E1 - OSPF external type 1, E2 - OSPF external type 2 i - IS-IS, L1 - IS-IS level-1, L2 - IS-IS level-2, ia - IS-IS inter area * - candidate default Routing table for VRF=0 C 10.0.0.0/24 is directly connected, port2 B 10.13.13.1/32 [20/0] via 172.16.42.3 (recursive is directly connected, port3), 01:06:48 C 172.16.42.2/31 is directly connected, port3 C 192.168.22.0/24 is directly connected, port1 FortiGate-VM64-KVM # get router info bgp network VRF 0 BGP table version is 1, local router ID is 172.16.42.2 Status codes: s suppressed, d damped, h history, * valid, \u0026gt; best, i - internal, S Stale Origin codes: i - IGP, e - EGP, ? - incomplete Network Next Hop Metric LocPrf Weight RouteTag Path *\u0026gt; 10.13.13.1/32 172.16.42.3 0 0 0 65500 i \u0026lt;-/1\u0026gt; Total number of prefixes 1 4.2. Scenario 2: One lb is down # Stop nginx on lb2 (ExaBGP only, Bird may require the complete shutdown) or stop lb2 physically. lb2$ sudo service nginx stop lb2$ sudo journalctl -fu exabgp -- Logs begin at Thu 2022-01-27 12:07:29 UTC. -- Feb 07 11:12:11 lb2 healthcheck[13584]: send announces for DOWN state to ExaBGP Feb 07 11:12:11 lb2 exabgp[13562]: api route added to neighbor 10.12.12.254 local-ip 10.12.12.2 local-as 65500 peer-as 65500 router-id 10.12.12.254 family-allowed in-open : 10.13.13.1/32 next-hop self med 1000 Check client client$ curl 10.13.13.1/hostname lb1 Check EdgeRouter, you may notice that the metric of 10.12.12.2 hop became 1000. FortiGate-VM64-KVM # get router info bgp network VRF 0 BGP table version is 4, local router ID is 172.16.42.3 Status codes: s suppressed, d damped, h history, * valid, \u0026gt; best, i - internal, S Stale Origin codes: i - IGP, e - EGP, ? - incomplete Network Next Hop Metric LocPrf Weight RouteTag Path *\u0026gt; 0.0.0.0/0 172.16.42.2 0 0 0 65000 i \u0026lt;-/1\u0026gt; * i10.13.13.1/32 10.12.12.2 1000 100 0 0 i \u0026lt;-/-\u0026gt; *\u0026gt;i 10.12.12.1 100 100 0 0 i \u0026lt;-/1\u0026gt; Total number of prefixes 2 Bring it back, then check client and EdgeRouter. FortiGate-VM64-KVM # get router info bgp network VRF 0 BGP table version is 5, local router ID is 172.16.42.3 Status codes: s suppressed, d damped, h history, * valid, \u0026gt; best, i - internal, S Stale Origin codes: i - IGP, e - EGP, ? - incomplete Network Next Hop Metric LocPrf Weight RouteTag Path *\u0026gt; 0.0.0.0/0 172.16.42.2 0 0 0 65000 i \u0026lt;-/1\u0026gt; *\u0026gt;i10.13.13.1/32 10.12.12.2 100 100 0 0 i \u0026lt;-/2\u0026gt; *\u0026gt;i 10.12.12.1 100 100 0 0 i \u0026lt;-/1\u0026gt; Total number of prefixes 2 Check client again. client$ curl 10.13.13.1/hostname lb2 "},{"id":3,"href":"/blog/posts/getting-started-tiling-wm-part-6-i3-rounded-corners/","title":"Getting Started Tiling Wm [Part 6] I3 Rounded Corners","section":"./posts/","content":"According to Reddis post and i3-gaps issue, it seems like a lot of people would like this. But Airblade - i3-gaps maintainer doesn\u0026rsquo;t like it. But nevermind, we still have two ways to achieve it.\n1. Rounded i3-gaps # Resloved have an awesome fork to implement rounded corners. I fork it again to keep it up-to-date with the upstream i3-gaps. You can check it here.\nInstall i3-gaps. # Dependencies sudo apt install git libxcb1-dev libxcb-keysyms1-dev libpango1.0-dev \\ libxcb-util0-dev libxcb-icccm4-dev libyajl-dev \\ libstartup-notification0-dev libxcb-randr0-dev \\ libev-dev libxcb-cursor-dev libxcb-xinerama0-dev \\ libxcb-xkb-dev libxkbcommon-dev libxkbcommon-x11-dev \\ autoconf libxcb-xrm0 libxcb-xrm-dev automake i3status \\ ninja-build meson libxcb-shape0-dev build-essential -y git clone https://github.com/ntk148v/i3.git cd i3/ # Compile mkdir -p build \u0026amp;\u0026amp; cd build meson .. ninja sudo ninja install Configure rounded corners by adding this to your config. border_radius 10 Result, but you can see that these corners look so jagged (check out this comment). 2. picom # You can also achieve rounded corners by using picom. Install picom. Configure rounded corners by changing corner_radius value. # Path ~/.config/picom.conf corner_radius = 10; Result looks much smoother. "},{"id":4,"href":"/blog/posts/rename-files-in-linux/","title":"Rename Files in Linux","section":"./posts/","content":" Rename a single file with mv. Just a basic thing. Rename multiple files with mv. # Rename files with suffix .yaml to yml for f in *.yaml; do mv -- \u0026#34;$f\u0026#34; \u0026#34;${f%.yaml}.yml\u0026#34; done Rename multiple files with rename. # Install rename command # Ubuntu/Debian-derived distros sudo apt install rename # RedHat-derived distros sudo yum install prename # The follow examples are performed in Ubuntu/Debian-derived distros rename \u0026#39;s/.yaml/.yml/\u0026#39; *.yaml # Replace all occurrences of \u0026#34;prev_\u0026#34; with \u0026#34;next_\u0026#34; rename \u0026#39;s/prev_/next_\u0026#39; *.c # Delete part of a filename rename \u0026#39;s/next_//\u0026#39; *.c # Limit changes to specific parts of filenames # Only change the files that start with \u0026#34;paramater\u0026#34; rename \u0026#39;s/^param/parameter/\u0026#39; *.c # Search with grouping # Replace all occurrences of \u0026#34;string\u0026#34; and \u0026#34;strong\u0026#34; rename \u0026#39;s/(str|stro)ng/strength\u0026#39; *.c # Use translations with rename # Force filenames to uppercase rename \u0026#39;y/a-z/A-Z\u0026#39; *.py # More? man rename "},{"id":5,"href":"/blog/posts/goignore/","title":"Goignore","section":"./posts/","content":" Goignore - A .gitignore wizard which gnerates .gitignore files from the command line for you. Inspired by joe 1. Features # No installation necessary - just use the binary. Works on Linux, Windows \u0026amp; MacOS. Interactive user interface with bubbletea: Pagination, Filtering, Help\u0026hellip; Supports all Github-supported .gitignore files. 2. Install # Download the latest binary from the Release page. It\u0026rsquo;s the easiest way to get started with goignore. Make sure to add the location of the binary to your $PATH. 3. Usage # Just run. chmod a+x goignore goignore At the first time, goignore will download the Gitignore templates from Github. It may take a few seconds (depend on your network).\nThe list of gitignore templates.\nShow help. Filter. Result, the current gitignore is updated. "},{"id":6,"href":"/blog/posts/getting-started-tiling-wm-part-1-i3/","title":"Getting Started with Tiling WM [Part 1] - I3","section":"./posts/","content":" Disclaimer\nI love customizing desktop. I make changes in my desktop everyday, make it look eye candy. My colleagues ask me how to make their desktop look like mine. But there are many steps and things to learn and follow, I know because I\u0026rsquo;ve gone throught it. Therefore I decide to write this getting-started guide to give people a shortest path to Fancy world.\n1. Overview Window Manager # First of all, you have to know the basic concepts.\n1.1. Desktop Environment vs. Window Manager # We\u0026rsquo;ll begin by showing how the Linux graphical desktop is layered. There are basically 3 layers that can be included in the Linux desktop:\nX Window: All GUIs require an X Window System layer, which draws graphic elements on the display. Without the X server, neither a WM nor a DE could create images on a Linux display. X also creates a framework for moving windows and performing tasks using a mouse and keyboard. Window manager: A WM controls the placement and appearance of screen elements. Popular WMs: i3, awesome, dwm\u0026hellip; Requires configuration to get what you want. Tends to be flexible and highly customizable. Customizes most aspects of a Linux experience. For Nerd: not user-friendly out of the box, need to edit configuration files\u0026hellip; Desktop environment: A DE requires both X and a WM. It also adds the deeper and seemless integration with applications, panels, system menus, status applets, drag-and-drop functionality,\u0026hellip; The most popular DEs: KDE Plasma, GNOME, Pantheon, Cinnamon,\u0026hellip; Sounds familiar, right? Excellent default configurations. May not be particularly customizable. Gret if you don\u0026rsquo;t want to customize everything. graph TD; A[Desktop Environment] --\u003e B[Window Manager]; B --\u003e C[X Windows]; 1.2. Types of Window Manager # Stack window manager: A stack window manager renders the window one-by-one onto the screen at specific co-orinates. If one window\u0026rsquo;s area overlaps another, then the window \u0026ldquo;on top\u0026rdquo; overwites part of the other\u0026rsquo;s visible appearance. This results in the appearance familiar to many users in which windows act a little like pieces of paper on a desktop, which can be moved around and allowed to overlap. Openbox, Fluxbox, Enlightenment\u0026hellip; Tiling window manager: A window manager with an organization of the screen into mutually non-overlapping frames (hence the name tiling), as opposed to the traditional approach of coordinate-based stacking of objects (windows) that tries to emulate the desk paradigm. Awesome,i3 (here we go), dwm, bspwm\u0026hellip; Compositing window manager: A compositing window manager may appear to the user similar to a stacking window manager. However, the individual windows are first renders in individual buffers, and then theirs images are composited onto the screen buffer; this two-step process means that visual effects (such as shadows, translucency) can be applied. Mutter (GNOME), Xfwm (XFCE), Compiz (Unity), KWin (KDE) 2. Overview I3 # i3wm is a tiling window manager designed for X11. It supports tiling, stacking, and tabbing layouts, which it handles dynamically. Configuration is achieved via plain text file. Keyboard navigation: Hall full control with keyboard navigation. Minimalistic: no window decorations or nonsense icons floating around. But you can fully customize it. Window management: is left to the user. Windows are held inside containers, which can be split veritically or horizontally. They can also optionally be resized. There are also options for stacking the windows, as well as tabbing them. Floating pop-up windows. Want more, check this. 3. Minimal I3 setup # 3.1. Operating System # Ubuntu 20.04 (Desktop/Server), download the installer and install Ubuntu by walking through installer. If you choose Ubuntu Server, you\u0026rsquo;ll need a display server so let\u0026rsquo;s install X Window System (Xorg). sudo apt install xinit # The configuration files are stored in /etc/X11/xinit # You can override it by creating and modifying ~/.xinitrc 3.2. Install I3 # You can install i3 from Ubuntu repository. It includes the window manager, a screen locker and two programs which write a status line to i3bar through stdout. Note that, i3-wm conflicts with i3-gaps (a fork of i3 with gaps and other features). sudo apt install i3 Or build from source, I have a personal fork - Rounded i3-gaps. # Dependencies sudo apt install git libxcb1-dev libxcb-keysyms1-dev libpango1.0-dev \\ libxcb-util0-dev libxcb-icccm4-dev libyajl-dev \\ libstartup-notification0-dev libxcb-randr0-dev \\ libev-dev libxcb-cursor-dev libxcb-xinerama0-dev \\ libxcb-xkb-dev libxkbcommon-dev libxkbcommon-x11-dev \\ autoconf libxcb-xrm0 libxcb-xrm-dev automake i3status \\ ninja-build meson libxcb-shape0-dev build-essential -y git clone https://github.com/ntk148v/i3.git cd i3/ # Compile mkdir -p build \u0026amp;\u0026amp; cd build meson .. ninja sudo ninja install Reboot. After reboot, choose i3 as your WM/DE.\nA prompt will be shown to create a config file, just hit \u0026lt;Enter\u0026gt;.\n\u0026lt;mod\u0026gt; key is Window now. Hit \u0026lt;mod\u0026gt;+Enter to start terminal emulator.\nWe have i3 first setup here with the default configuration. As mentioned before, configuration is achieved via plain text file.\n4. Usage # This post doesn\u0026rsquo;t aim to cover everything about i3, see the official documentation for more information.\n4.1. Keybindings # In i3, commands are invoked with a modifier key, referred to as $mod. This is Alt (Mod1) by default, with Super (Mod4) being a popular alternative. Super is the key usually represented on a keyboard as a Windows icon, or on an Apple keyboard as a Command key. See i3 reference card and Using i3 for defaults. 4.2. Workspace, Container and Window # graph TD; i3--\u003eWorkspace1; i3--\u003eWorkspace2; Workspace1--\u003eContainer1; Workspace1--\u003eContainer2; Container1--\u003eWindow1; Container2--\u003eWindow2; Workspace2--\u003eContainer3; Container3--\u003eWindow3; Container3--\u003eWindow4; style i3 fill:#33ddaa; style Workspace1 fill:#ea9999; style Workspace2 fill:#ea9999; style Container1 fill:#6fa8dc; style Container2 fill:#6fa8dc; style Container3 fill:#6fa8dc; In i3, workspace is simply the equivalent of a virtual desktop. You can have as many workspaces as you want. i3 manages windows in a tree structure, with containers as building blocks. A container contains one or multiple windows. Its windows will be positioned depending on the container\u0026rsquo;s layout. There are 3 different layouts possible: Split: Each window share the container space and are split horizontally (splith) or vertically (splitv). This is the default layout. Stacked - The focused window is visible and the other ones are stacked behind. You can change the window’s focus via keystrokes easily. You have access to the list of windows open too, at the top of the container itself. Tabbed - This layout is similar as the stacked layout, except that the windows’ list is vertically split, and not horizontally. A window, where an application is running, can be created in a container. It will automatically position itself and be in focus, depending on the container’s layout. You can move them around or even change the layout of the container using keystrokes. There are two different sorts of windows: fixed windows (by default) and floating windows. 4.3. Application launcher # i3 uses dmenu as an application launcher, which is bound by default to $mod+d. rofi is a popular dmenu replacement and more that can list dekstop entries. 5. Configuration # You can use my minimal configuration. It requires some extra packages. sudo apt install hsetroot rofi compton -y Minimal I3 configuration Cheat sheat shortcut # Shortcut Description $mod + Enter Open a terminal $mod + d Open application launcher $mod + c Kill focused window $mod + right/left/up/down Change focus $mod + Shift + right/left/up/down Move focused window in direction $mod + 1/2/3\u0026hellip; Switch workspace $mod + Shift + 1/2/3\u0026hellip; Move focused window to the specified workspace $mod + space Change between focus and floating mode $mod + Shift + r Restart i3 $mod + Shift + c Reload config i3 $mod + q Quit i3 Minimal configuration file # # i3 config file # Please see https://i3wm.org/docs/userguide.html for a complete reference! # Set modifier set $mod Mod4 set $alt Mod1 default_orientation auto # Font for window titles. font pango:DejaVu Sans Mono 0 # Use Mouse+$mod to drag floating windows to their wanted position floating_modifier $mod # Here is the trick to disable titlebar completely for_window [class=\u0026#34;^.*\u0026#34;] border pixel 0 # Autostart exec --no-startup-id hsetroot -solid \u0026#34;#F1CCBB\u0026#34; exec --no-startup-id xsettingsd \u0026amp; exec --no-startup-id compton -b exec --no-startup-id xss-lock --transfer-sleep-lock -- i3lock --nofork exec --no-startup-id nm-applet # Keybindings # start a terminal bindsym $mod+Return exec i3-sensible-terminal # kill focused window bindsym $mod+c kill bindsym $alt+F4 kill # start rofi bindsym $mod+Shift+d exec i3-dmenu-desktop --dmenu=\u0026#34;dmenu -i -fn \u0026#39;DejaVu Sans:size=8\u0026#39;\u0026#34; bindsym $mod+d exec rofi -lines 12 -padding 18 -width 60 -location 0 -show drun -sidebar-mode -columns 3 -font \u0026#39;DejaVu Sans 8\u0026#39; # Use pactl to adjust volume in PulseAudio. set $refresh_i3status killall -SIGUSR1 i3status bindsym XF86AudioRaiseVolume exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ +10% \u0026amp;\u0026amp; $refresh_i3status bindsym XF86AudioLowerVolume exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ -10% \u0026amp;\u0026amp; $refresh_i3status bindsym XF86AudioMute exec --no-startup-id pactl set-sink-mute @DEFAULT_SINK@ toggle \u0026amp;\u0026amp; $refresh_i3status bindsym XF86AudioMicMute exec --no-startup-id pactl set-source-mute @DEFAULT_SOURCE@ toggle \u0026amp;\u0026amp; $refresh_i3status # change focus bindsym $mod+Left focus left bindsym $mod+Down focus down bindsym $mod+Up focus up bindsym $mod+Right focus right # alternatively, you can use the cursor keys: bindsym $mod+Shift+Left move left bindsym $mod+Shift+Down move down bindsym $mod+Shift+Up move up bindsym $mod+Shift+Right move right # split in horizontal orientation bindsym $mod+h split h # split in vertical orientation bindsym $mod+v split v # enter fullscreen mode for the focused container bindsym $mod+f fullscreen toggle # change container layout (stacked, tabbed, toggle split) bindsym $mod+s layout stacking bindsym $mod+w layout tabbed bindsym $mod+e layout toggle split # toggle tiling / floating bindsym $mod+Shift+space floating toggle # change focus between tiling / floating windows bindsym $mod+space focus mode_toggle # focus the parent container bindsym $mod+a focus parent # Define names for default workspaces for which we configure key bindings later on. # We use variables to avoid repeating the names in multiple places. set $ws1 \u0026#34;1\u0026#34; set $ws2 \u0026#34;2\u0026#34; set $ws3 \u0026#34;3\u0026#34; set $ws4 \u0026#34;4\u0026#34; set $ws5 \u0026#34;5\u0026#34; set $ws6 \u0026#34;6\u0026#34; set $ws7 \u0026#34;7\u0026#34; set $ws8 \u0026#34;8\u0026#34; set $ws9 \u0026#34;9\u0026#34; set $ws10 \u0026#34;10\u0026#34; # switch to workspace bindsym $mod+1 workspace number $ws1 bindsym $mod+2 workspace number $ws2 bindsym $mod+3 workspace number $ws3 bindsym $mod+4 workspace number $ws4 bindsym $mod+5 workspace number $ws5 bindsym $mod+6 workspace number $ws6 bindsym $mod+7 workspace number $ws7 bindsym $mod+8 workspace number $ws8 bindsym $mod+9 workspace number $ws9 bindsym $mod+0 workspace number $ws10 # move focused container to workspace bindsym $mod+Shift+1 move container to workspace number $ws1 bindsym $mod+Shift+2 move container to workspace number $ws2 bindsym $mod+Shift+3 move container to workspace number $ws3 bindsym $mod+Shift+4 move container to workspace number $ws4 bindsym $mod+Shift+5 move container to workspace number $ws5 bindsym $mod+Shift+6 move container to workspace number $ws6 bindsym $mod+Shift+7 move container to workspace number $ws7 bindsym $mod+Shift+8 move container to workspace number $ws8 bindsym $mod+Shift+9 move container to workspace number $ws9 bindsym $mod+Shift+0 move container to workspace number $ws10 # reload the configuration file bindsym $mod+Shift+c reload # restart i3 inplace (preserves your layout/session, can be used to upgrade i3) bindsym $mod+Shift+r restart # exit i3 (logs you out of your X session) bindsym $mod+Shift+e exec \u0026#34;i3-nagbar -t warning -m \u0026#39;You pressed the exit shortcut. Do you really want to exit i3? This will end your X session.\u0026#39; -B \u0026#39;Yes, exit i3\u0026#39; \u0026#39;i3-msg exit\u0026#39;\u0026#34; # resize window (you can also use the mouse for that) mode \u0026#34;resize\u0026#34; { bindsym Left resize shrink width 5 px or 5 ppt bindsym Down resize grow height 5 px or 5 ppt bindsym Up resize shrink height 5 px or 5 ppt bindsym Right resize grow width 5 px or 5 ppt bindsym Return mode \u0026#34;default\u0026#34; } bindsym $mod+r mode \u0026#34;resize\u0026#34; # panel bar { colors { background #2f343f statusline #2f343f separator #4b5262 # colour of border, background, and text focused_workspace #2f343f #bf616a #d8dee8 active_workspace #2f343f #2f343f #d8dee8 inactive_workspace #2f343f #2f343f #d8dee8 urgent_workspacei #2f343f #ebcb8b #2f343f } status_command i3status } # gaps gaps top 20 gaps left 10 gaps right 10 gaps bottom 10 gaps inner 25 border_radius 10 # Hide edge borders only if there is one window with no gaps hide_edge_borders smart_no_gaps # reload exec_always hsetroot -solid \u0026#34;#F1CCBB\u0026#34; 6. Some tricks and tips # A trick with terminal emulator:\nDisable scrollbar and menubar. Configure padding for vte-terminal. mkdir -p ~/.config/gtk-3.0 cat \u0026lt;\u0026lt;EOT \u0026gt;\u0026gt; ~/.config/gtk-3.0/gtk.css vte-terminal { padding: 30px; } EOT Result. 7. References # https://www.lifewire.com/window-manager-vs-the-desktop-environment-in-linux-4588338 https://en.wikipedia.org/wiki/X_window_manager https://i3wm.org/docs https://wiki.archlinux.org/title/i3 "},{"id":7,"href":"/blog/posts/getting-started-tiling-wm-part-2-rofi/","title":"Getting Started with Tiling WM [Part 2] - Rofi","section":"./posts/","content":" In the part1, I\u0026rsquo;ve used rofi instead of dmenu. This part will show you how to start with rofi. 1. Introduction # Rofi is a window switcher, application launcher and dmenu replacement. Features: Fully configurable keyboard navigation. Type to filer. Built-in modes: Window switcher mode. Application laucher. Desktop file application launcher. SSH laucher mode. History-based ordering. \u0026hellip; 2. Getting started # Installing rofi is quite easy. sudo apt install rofi -y Run it for the first time. rofi -lines 12 -padding 18 -width 60 -location 0 -show drun -sidebar-mode -columns 3 -font \u0026#39;DejaVu Sans 8\u0026#39; -show mode Open rofi in a certain mode. Available modes are window, run, drun, ssh, combi. The special argument keys can be used to open a searchable list of supported key bind‐ ings (see KEY BINDINGS) To show the run-dialog: rofi -show run -lines Maximum number of lines to show before scrolling. rofi -lines 25 Default: 15 -location Specify where the window should be located. The numbers map to the following loca‐ tions on screen: 1 2 3 8 0 4 7 6 5 Default: 0 -padding Define the inner margin of the window. Default: 5 -sidebar-mode Open in sidebar-mode. In this mode a list of all enabled modes is shown at the bot‐ tom. (See -modi option) To show sidebar, use: rofi -show run -sidebar-mode -lines 0 Press hot key (defined in i3 configuration file) \u0026lt;Window\u0026gt;+d to start rofi. Use \u0026lt;Shift\u0026gt;+\u0026lt;left/right\u0026gt; to switch between mode. More details you can found in rofi github. 3. Tweaking # The default setup looks quite boring. Let\u0026rsquo;s tweak a bit! There are currently three methods of setting configuration options: Local configuration. Normally, depending on XDG, in ~/.config/rofi/config. This uses the Xresources format. Xresources: A method of storing key values in the Xserver. See here for more information. Command line options: Arguments are passed to Rofi. We will use configuration file. mkdir -p ~/.config/rofi/themes touch ~/.config/rofi/themes/onedark.theme Copy the follow content into ~/.config/rofi/themes/onedark.theme: configuration { show-icons: true; font: \u0026#34;DejaVu Sans Mono 10\u0026#34;; modi: \u0026#34;window,run,drun\u0026#34;; } * { background: #282c34; foreground: #abb2bf; background-color: @background; selected-normal-foreground: @foreground; selected-normal-background: #98c379; selected-urgent-background: #e5c07b; selected-urgent-foreground: @foreground; border: 5; lines: 12; padding: 0; margin: 0; spacing: 0; } window { width: 50%; transparency: \u0026#34;real\u0026#34;; } mainbox { children: [inputbar, listview]; } listview { columns: 1; } element { padding: 12; orientation: vertical; text-color: @foreground; } element selected { background-color: @selected-normal-background; text-color: @background; } inputbar { background-color: @background; children: [prompt, entry]; } prompt { enabled: true; font: \u0026#34;DejaVu Sans Mono 10\u0026#34;; padding: 12 0 0 12; text-color: @selected-urgent-background; } entry { padding: 12; text-color: @selected-urgent-background; } Run it and you can see the magic! rofi -theme ~/.config/rofi/themes/onedark.theme -show drun Don\u0026rsquo;t forget to update hotkey in i3 configuration file. bindsym $mod+d exec rofi -theme ~/.config/rofi/themes/onedark.theme -show drun "},{"id":8,"href":"/blog/posts/getting-started-tiling-wm-part-3-polybar/","title":"Getting Started with Tiling WM [Part 3] - Polybar","section":"./posts/","content":" 1. Overview # According to Polybar frontpage, Polybar is A fast and easy to use tool for creating status bars .\nWIP "},{"id":9,"href":"/blog/posts/getting-started-tiling-wm-part-4-xresources/","title":"Getting Started with Tiling WM [Part 4] - Xresources","section":"./posts/","content":" WIP "},{"id":10,"href":"/blog/posts/getting-started-tiling-wm-part-5-compton/","title":"Getting Started with Tiling WM [Part 5] - Compton","section":"./posts/","content":" WIP "},{"id":11,"href":"/blog/posts/linux-tools-that-you-never-knew-you-needed/","title":"Linux tools that you never knew you needed","section":"./posts/","content":" 1. bat - (cat alternative) # bat: A cat(1) clone with syntax highlighting and Git integration. Example: 2. fd - (find alternative) # fd: a simple, fast and user-friendly alternative to find. Examples: 3. httpie - (wget/curl alternative) # httpie: a user-friendly command-line HTTP client for the API era. It comes with JSON support, syntax highlighting, persistent sessions, wget-like downloads, plugins, and more. Examples: # Hello world $ http httpie.io/hello # Custom HTTP method, HTTP headers and JSON data $ http PUT pie.dev/put X-API-Token:123 name=John # Submitting forms $ http -f POST pie.dev/post hello=World # Upload a file using redirect input $ http pie.dev/post \u0026lt; files/data.json # ... # For more examples, check out: https://httpie.io 4. ripgrep - (grep alternative) # ripgrep: a faster grep. ripgrep is a line-oriented search tool that recursively searches your current directory for a regex pattern. By default, ripgrep will respect your .gitignore and automatically skip hidden files/directories and binary files. Benchmark. Examples: # Basic use $ rg fast README.md # Regular expressions $ rg \u0026#39;fast\\w+\u0026#39; README.md # Recursive search - recursively searching the directory (current directory is default) $ rg \u0026#39;fn write\\(\u0026#39; # ... # For more examples, checkout: https://github.com/BurntSushi/ripgrep/blob/master/GUIDE.md 5. delta # delta: Code evolves, and we all spend time studying diffs. Delta aims to make this both efficient and enjoyable: it allows you to make extensive changes to the layout and styling of diffs, as well as allowing you to stay arbitrarily close to the default git/diff output. Diff/git diff doesn\u0026rsquo;t show you exactly what was changed. Delta shows within-line highlights based on a Levenshtein edit inference algorithm. By default, delta restructures the git output slightly to make the hunk markers human-readable: Example config: [core] pager = delta [delta] plus-style = \u0026#34;syntax #98c379\u0026#34; minus-style = \u0026#34;syntax #e06c75\u0026#34; syntax-theme = OneHalfDark navigate = true features = line-numbers decorations [interactive] diffFilter = delta --color-only Completely replace diff with delta: alias diff=\u0026#34;delta\u0026#34; 6. z # Tired for cding into the same directories over and over? Save your time with z command! z: jump around. Z is a shell script that makes jumping around your file directory pleasantly simple. Instead of trying to remember the exact path of where you need to go, or worse, cding into the next directory followed by lsing and then cding again over and over (we’ve all been there), Z allows you to “lazy type” where you want to go and it’ll handle the rest. Examples: # Takes me to my workspace folder from anywhere. $ z workspace 7. fzf # fzf: fzf is a general-purpose command-line fuzzy finder. It\u0026rsquo;s an interactive Unix filter for command-line that can be used with any list; files, command history, processes, hostnames, bookmarks, git commits, etc. Examples: # Read the list from STDIN and write the selected item to STDOUT $ find * -type f | fzf \u0026gt; selected $ vim $(fzf) # COMMAND **\u0026lt;TAB\u0026gt; # Files under the current directory # - You can select multiple items with TAB key $ vim **\u0026lt;TAB\u0026gt; # Files under parent directory $ vim ../**\u0026lt;TAB\u0026gt; # Files under parent directory that match `fzf` $ vim ../fzf**\u0026lt;TAB\u0026gt; # Files under your home directory $ vim ~/**\u0026lt;TAB\u0026gt; # Directories under current directory (single-selection) $ cd **\u0026lt;TAB\u0026gt; # Directories under ~/github that match `fzf` $ cd ~/github/fzf**\u0026lt;TAB\u0026gt; # Can select multiple processes with \u0026lt;TAB\u0026gt; or \u0026lt;Shift-TAB\u0026gt; keys $ kill -9 \u0026lt;TAB\u0026gt; $ unset **\u0026lt;TAB\u0026gt; $ export **\u0026lt;TAB\u0026gt; $ unalias **\u0026lt;TAB\u0026gt; # ... # For more examples, checkout: https://github.com/junegunn/fzf 8. thefuck # thefuck: Magnificent app which corrects your previous console command. Examples: $ apt-get install vim E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied) E: Unable to lock the administration directory (/var/lib/dpkg/), are you root? $ fuck sudo apt-get install vim [enter/↑/↓/ctrl+c] Reading package lists... Done ... # ... # For more examples, check out: https://github.com/nvbn/thefuck 9. exa - (ls alternative) # exa: A modern replacement for ls. exa is an improved file lister with more features and better defaults. It uses colours to distinguish file types and metadata. It knows about symlinks, extended attributes, and Git. And it’s small, fast, and just one single binary. Examples: Completely replace ls with exa: alias ls=\u0026#34;exa\u0026#34; "},{"id":12,"href":"/blog/posts/set-animated-gif-as-wallpaper/","title":"Set Animated Gif as Wallpaper","section":"./posts/","content":" NOTE: Environment Ubuntu 20.04\nDependencies # Xwinwrap: sudo apt-get install xorg-dev build-essential libx11-dev x11proto-xext-dev libxrender-dev libxext-dev git clone https://github.com/ujjwal96/xwinwrap.git cd xwinwrap make sudo make install make clean Gifsicle: sudo apt install gifsicle The helper script # A helper script to setup animated .gif in dual monitors.\n#!/bin/bash # Uses xwinwrap to display given animated .gif in dual monitors. if [ $# -ne 1 ]; then echo 1\u0026gt;\u0026amp;2 Usage: $0 image.gif exit 1 fi gif=$1 killall -9 xwinwrap killall -9 gifview # Get monitors resolution SCR1=`xrandr | awk \u0026#39;/primary/ \u0026amp;\u0026amp; /connected/ { print $4 }\u0026#39;` SCR2=`xrandr | awk \u0026#39;!/primary/ \u0026amp;\u0026amp; /connected/ { print $3 }\u0026#39;` xwinwrap -g $SCR1 -ov -ni -s -nf -- gifview -w WID $gif -a \u0026amp; xwinwrap -g $SCR2 -ov -ni -s -nf -- gifview -w WID $gif -a \u0026amp; If you want to run xwinwrap by yourself, here is the example:\nxwinwrap -g 1920x1080 -ov -ni -s -nf -- gifview -w WID /full/path/to/gif -a "},{"id":13,"href":"/blog/posts/linux-swap-space-note/","title":"Swap space note","section":"./posts/","content":" 1. What is Swap? # Swap file systems support virtual memory, data is written to a swap file system when there is not enough RAM to store the data your system is processing.\n2. Swap partition size # 2.1. Old rule of thumb # swap = 2 * the-amount-of-RAM So if a computer had 64KB of RAM, a swap partition of 128KB would be an optimum size. This rule took into the facts that RAM sizes were typically quite small at the time. Nowadays, RAM has become a cheap \u0026amp; affordable commondity, so the 2x rule is outdated.\n2.2. What is the right amount of swap space? # Choosing the correct swap size is important. Too much swap space can hide memory leaks, also the storage space is allocated but idle. It can affect the system performance in general.\nFollow the RedHat (CentOS 7x \u0026amp; RHEL 7) guide, the recommended size of a swap partition depending on the amount of RAM \u0026amp; whether you want sufficient memory for your system.\nswap \u0026lt;= 10% * total-size-hard-drives \u0026amp;\u0026amp; swap \u0026lt;= 128GB (if hibernation is allowed) Amount of RAM Recommended swap space Recommended swap space if allowing for hibernation \u0026lt; 2GB 2 * the-amount-of-RAM 3 * the-amount-of-RAM \u0026gt; 2GB - 8GB the-amount-of-RAM 2 * the-amount-of-RAM \u0026gt; 8GB - 64GB \u0026gt;= 4GB 1.5 * the-amount-of-RAM \u0026gt; 64GB \u0026gt;= 4GB Hibernation not recommended 3. Common misconceptions \u0026amp; gotchas # 3.1. Increasing swap size would increase performance # No, it wouldn\u0026rsquo;t. Remember that the slowest part of memory is your hard-disk - swap just provides the ability to use more memory by swapping some pages out to the disk, which is slow compared to RAM operations. Swap can also increase disk I/O \u0026amp; CPU load. This is a tradeoff. Without swap, the OOM may get you. It causes a downtime and in the real life scenario, the application can be slow a bit rather than down completely. 3.2. Swappiness # The linux kernel tunable parameter vm.swappiness (/proc/sys/vm/swappiness) can be used to define how aggressively memory pages are swapped to disk.\nThe default value: 60. The lower the value, the less swapping is used \u0026amp; the more memory pages are kept in the physical memory.\n* 0: swap is disable. * 1: minimum amount of swapping without disabling it entirely. * 10: recommended value to improve performance when sufficient memory exists in a system * 100: aggressive swapping Useful commands:\n# Check the current value sysctl vm.swappiness # Adjust the value echo 10 \u0026gt; /proc/sys/vm/swappiness sysctl -w vm.swappiness=10 echo \u0026#34;vm.swappiness = 10\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf On SSDs, swapping out anonymous pages and reclaiming file pages are essentially equivalent in terms of performance/latency. On older spinning disks, swap reads are slower due to random reads, so a lower vm.swappiness setting makes sense there.\n3.3. Using swap as emergency memory # Swap is not generally about getting emergency memory, it\u0026rsquo;s about making memory reclamation egalitarian and efficient. In fact, using it as \u0026ldquo;emergency memory\u0026rdquo; is generally actively harmful. 4. References # RedHad guideline\nChris Down\u0026rsquo;s post\nLinux Hint - Understanding vm.swappiness\n"},{"id":14,"href":"/blog/posts/ansitheus/","title":"Ansitheus","section":"./posts/","content":"Ansitheus = Ansible + Prometheus 1. Prometheus overview # NOTE: Checkout the Prometheus official documentation.\nPrometheus is an open-source systems monitoring \u0026amp; alerting toolkit originally built at SoundCloud.\n1.1. Features # a multi-dimensional data model with time series data identified by metric name \u0026amp; key/value pairs PromQL, a flexible query language to leverage this dimensionality no reliance on distributed storage; single server nodes are autonomous time series collection happens via a pull model over HTTP pushing time series is supported via an intermediary gateway targets are discovered via service discovery or static configuration multiple modes of graphing \u0026amp; dashboarding support 1.2. Architecture \u0026amp; components # Prometheus scrapes metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. It stores all scraped samples locally \u0026amp; runs rules over this data to either aggregate \u0026amp; record new time series from existing data or generate alerts. Grafana or other API consumers can be used to visualize the collected data.\nPrometheus server: scrapes \u0026amp; stores time series data. Prometheus alertmanager: handle alerts. Special-purpose exporters. Push-gateway: support short-lived jobs. client libraries: instrument application code. Various support tools: Grafana,\u0026hellip; 2. Ansitheus # 2.1. Why Ansitheus? # As you can see that, Prometheus ecosystem consists of multiple components. The operator may need a lot of efforts to configure, deploy \u0026amp; maintain these components. To make life easier, it is necessary to enter the world of automation, using modern tools of configuration management, provisioning \u0026amp; orchestration. Ansible is one of them. It is simple, agentless IT automation that anyone can use. My team decided to choose it as the automation solution, \u0026amp; Ansitheus is the result.\n2.2. Features # The idea using Ansible to deploy Prometheus is not new. There are many existing solutions:\ncloudalchemy/ansible-prometheus ernestas-poskus/ansible-prometheus \u0026hellip; So what makes Ansitheus be different with others?\nDeploy, configure \u0026amp; maintain the Prometheus ecosystem easily. Allow users to configure, deploy the system from scratch: Prepare, configure the local repository. Install the necessary packages. Configure Docker private registry. Configure Docker daemon. Containerize Prometheus components: Docker is hotter than hot because it makes it possible to get far more apps running on the same old servers \u0026amp; it also makes it very easy to package \u0026amp; ship programs. You can easily find the advantages of Docker \u0026amp; container through internet. High availability with HAProxy \u0026amp; Keepalived. Support centralized Docker logging with Fluentd. Support Ansible vault to work with sensitive data. 2.3. Components # Prometheus Server Prometheus Alertmanager Prometheus Node-exporter Google Cadvisor Prometheus SNMP exporter Haproxy Keepalived Fluentd Grafana Other Prometheus exporters - TODO 2.4. Getting started # Install Ansible in deployment node.\nClone this repostiory.\nCreate configuration directory, default path /etc/ansitheus.\nsudo mkdir -p /etc/ansitheus sudo chown $USER:$USER /etc/ansitheus Copy config.yml to /etc/ansitheus directory - this is the main configuration for Ansible monitoring tool.\ncp /path/to/ansitheus/repository/etc/ansitheus/config.yml \\ /etc/ansitheus/config.yml Copy inventory files to the current directory.\ncp /path/to/ansitheus/repository/ansible/inventory/* . Modify inventory \u0026amp; /etc/ansitheus/config.yml.\nRun tools/ansitheus, figure out yourself:\nUsage: ./tools/ansitheus COMMAND [option] Options: --inventory, -i \u0026lt;inventory_path\u0026gt; Specify path to ansible inventory file --configdir, -c \u0026lt;config_path\u0026gt; Specify path to directory with config.yml --verbose, -v Increase verbosity of ansible-playbook --tags, -t \u0026lt;tags\u0026gt; Only run plays \u0026amp; tasks tagged with these values --help, -h Show this usage information --skip-common Skip common role --ask-vault-pass Ask for vault password Commands: precheck Do pre-deployment checks for hosts deploy Deploy \u0026amp; start all ansitheus containers pull Pull all images for containers (only pull, no running containers) destroy Destroy Prometheus containers \u0026amp; service configuration --include-images to also destroy Prometheus images --include-volumes to also destroy Prometheus volumes 2.5. Contributors # Kien Nguyen Dat Vu Duc Nguyen Long Cao "},{"id":15,"href":"/blog/posts/operate-etcd-cluster/","title":"Operate Etcd cluster","section":"./posts/","content":" NOTE: This is my perspective aggregation. You can easily find these such of knowledges in the references.\n1. Context # Etcd Version v3.4.0.\n2. Requirements # 2.1. Number of nodes # \u0026gt;= 3 nodes. A etcd cluster needs a majority of nodes, a quorum to agree on updates to the cluster state. For a cluster with n-members, quorum is (n/2)+1. 2.2. CPUs # Etcd doesn\u0026rsquo;t require a lot of CPU capacity. Typical clusters need 2-4 cores to run smoothly. 2.3. Memory # Etcd performance depends on having enough memory (cache key-value data, tracking watchers\u0026hellip;). Typical 8GB is enough. 2.4. Disk # An etcd cluster is very sensitive to disk latencies. Since etcd must persist proposals to its log, disk activity from other processes may cause long fsync latencies. The upshot is etcd may miss heartbeats, causing request timeouts and temporary leader loss. An etcd server can sometimes stably run alongside these processes when given a high disk priority. Check whether a disk is fast enough for etcd using fio. If the 99th percentile of fdatasync is \u0026lt;10ms, your storage is ok. $ fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data \\ --size=22m --bs=2300 --name=mytest SSD is recommended. 2.5. Network # Etcd cluster should be deployed in a fast and reliable network. Low latency ensures etcd members can communicate fast. High bandwidth can reduce the time to recover a failed etcd member. 1GbE is sufficient for common etcd. Note that the network isn\u0026rsquo;t the only source of latency. Each request and response may be impacted by slow disks on both the leader and followers. 3. Tuning # 3.1. Time parameters # Heartbeat interval. The frequency with which the leader will notify followers that it is still the leader. Default: 100ms. Best practice: Around 0.5-1.5 x round-trip time (RTT) between members. Measure RTT with ping. Tradeoff: Too low -\u0026gt; etcd will send unnecessary messages -\u0026gt; increase the usage of CPU and network resources. Too high -\u0026gt; leads to high election timeout. Election timeout. How long a follower node will go without hearing a heartbeat before attempting to become leader itself. Default: 1000ms. Best practice: \u0026gt;= 10 x RTT and \u0026lt; 50s. The heartbeat interval and election timeout value should be the same for all members in one cluster. # Command line arguments: $ etcd --heartbeat-interval=100 --election-timeout=500 # Environment variables: $ ETCD_HEARTBEAT_INTERVAL=100 ETCD_ELECTION_TIMEOUT=500 etcd 3.2. Disk # An etcd server can sometimes stably run alongside these processes when given a high disk priority using ionice. # best effort, highest priority $ sudo ionice -c2 -n0 -p `pgrep etcd` 3.3. Snapshot # etcd appends all key changes to a log file -\u0026gt; huge log that grows forever ☝️ Solution: Make periodic snapshots (save the current and remove old logs). Default: make snapshots after every 10 000 changes. Tuning: Just in case that etcd\u0026rsquo;s memory and disk usage is too high, lower threshold. # Command line arguments: $ etcd --snapshot-count=5000 # Environment variables: $ ETCD_SNAPSHOT_COUNT=5000 etcd 4. Maintenance # 4.1. History compaction # Etcd keeps an exact history of its keyspace, the history should be periodically compacted to avoid performance degradation and eventual storage space exhaustion. Etcd can be set to automatically compact the keyspace with the --auto-compaction-* option with a period of hours. # keep one hour of history $ etcd --auto-compaction-retention=1 --auto-compaction-mode=periodic Compaction modes: Revision-based: --auto-compaction-mode=revision --auto-compaction-retention=1000 automatically Compact on \u0026ldquo;latest revision\u0026rdquo; - 1000 every 5-minute (when latest revision is 30000, compact on revision 29000). Use this when having a large keyspace. Periodic: --auto-compaction-mode=periodic --auto-compaction-retention=72h automatically Compact with 72-hour retention window every 1-hour. Use this when having a huge number of revisions for a key-value pair. 4.2. Defragmentation # Compacting old revisions internally fragments etcd by leaving gaps in backend database - internal fragmentation. Internal fragmentation space is available for use by etcd but unavailable to the host filesystem. Solution: Release this space back to the filesystem with defrag. $ etcdctl defrag It should be run rather infrequently, as there is always going to be an unavoidable pause. 5. References # Etcd hardware: https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md Etcd tuning: https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md Etcd maintainence: https://etcd.io/docs/v3.4.0/op-guide/maintenance/ "},{"id":16,"href":"/blog/posts/golang-block-forever/","title":"Golang: Block forever","section":"./posts/","content":"Sometimes, you want to block the current goroutine when allowing others to continue. Here is some tricks I\u0026rsquo;ve collected:\n1. References # Firstly give them some credits:\nhttps://blog.sgmansfield.com/2016/06/how-to-block-forever-in-go/ https://pliutau.com/different-ways-to-block-go-runtime-forever/ NOTE: I run these with Golang 1.12\n2. The original # package main import \u0026#34;fmt\u0026#34; func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(1000) fmt.Println(i) } } func main() { go show() // The main goroutine is exited before the show() be done. fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 3. Bad - An empty infinite loop # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func forever() { for { // Empty, just do nothing } } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } An infinite loop here is a busy loop that does nothing except burn CPU time.\n4. Good - Busy blocking # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func forever() { for { runtime.Gosched() } } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } It will reduce your CPU usage but it isn\u0026rsquo;t the preferable solution.\n5. Good - Waiting on itself # We wait but we never done XD\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func forever() { wg := sync.WaitGroup{} wg.Add(1) wg.Wait() } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 6. Good - Empty select # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func forever() { select{ } } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 7. Good - Double locking # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func forever() { m := sync.Mutex{} // Same with sync.RWMutex m.Lock() m.Lock() } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 8. Good - Reading an Empty Channel # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func forever() { c := make(chan struct{}) \u0026lt;-c } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 9. Good - Self produce-and-consume # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func forever() { c := make(chan struct{}, 1) for { select { case \u0026lt;-c: case c \u0026lt;- struct{}{}: } } } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } "},{"id":17,"href":"/blog/posts/rip-kobe-bryant/","title":"Rest In Peace Kobe Bryant","section":"./posts/","content":"\nSource: The Undefeated\nI am not Kobe fan honestly but just like him, basketball is something I love. I hope he\u0026rsquo;s at peace because although his journey in life is over, the legacy he left behind is etched in all our souls.\nRest In Peace Mamba 🏀\n"},{"id":18,"href":"/blog/posts/target-2020/","title":"Mục tiêu 2020","section":"./posts/","content":" Tổng kết 2019 # Mục tiêu 2019:\nViết mục tiêu cho 2019 Kỹ năng: Sử dụng thành thạo và ứng dụng Golang vào project thực tết. \u0026gt;=1000 commits trên Github. Hey yooooooo! 🎉 Học về cách thiết kế một hệ thống phân tán giải quyết bài toán thật -\u0026gt; Vẫn nửa vời :( Đưa Kubernetes vào ứng dụng thực tế. Củng cố thêm kiến thức đã có hiện tại, nhưng chưa đủ 😢 Thất bại hoàn toàn: Dành thêm thời gian OpenStack (Kolla-Ansible, Zun,\u0026hellip;) Sách: 20-30 cuốn sách (sách gì cũng được, truyện trinh thám, văn học kinh điển, tech book). Du lịch: Quảng Bình - Hải Phòng - Thượng Hải\u0026hellip; Thể thao: Một câu chuyện buồn\u0026hellip; Blog (là cái này nè): Buồn tiếp. Nhạc nhẽo: Buồn tiếp^2, mới tập sơ sơ ukulele, chưa ưng ý. Chụp ảnh: Buồn tiếp^3. via GIPHY\nQua một năm viết ra những mục tiêu, mình nhận thấy một điều rằng không chỉ mình lười, mình còn chẳng biết thực sự mình sống và làm việc để hướng đến điều gì. Vật chất, hay hiện thực hóa chính là tiền, nhà cửa, xe cộ, mình có thể đạt được không sớm thì muộn. Nhưng sau đó sẽ hướng đến gì tiếp theo? via GIPHY\nNhưng nếu bạn còn không biết bản thân muốn gì vậy làm thế nào để theo đuổi?\nMục tiêu 2020 # via GIPHY\nViết tạm ra đây trước, trước tiên mình phải tìm được câu trả lời cho câu hỏi phía trên đã.\nKỹ năng: Phát huy những gì đã làm được từ năm trước. Học về kỹ năng căn bản tốt hơn, ít nhất là tốt cho interview. Du lịch: Miền Tây Nam Bộ, yup!!!! Lào? (Maybe) Phan Rang/Lý Sơn. Huế. Hải Phòng. Sách: Đọc thêm sách Tech. Thể thao: Tiếp tục cố gắng chơi bóng rổ. Ballin\u0026rsquo; 🏀 Tiếp tục tập Workout - giữ vững nhịp tập thể dục buổi chiều. Chụp ảnh: Cần xác định rõ liệu còn yêu thích nhiếp ảnh không? Hay chỉ là sở thích nhất thời. Tiếng Anh: Tiếng Anh mình thật sự tệ, cần phải đi học tiếng Anh nghiêm túc. Ít nhất là tiếng Anh giao tiếp, tránh việc nói chuyện bị khớp. Công việc: Suy nghĩ thực sự về một cơ hội phát triển bản thân khác. Có thể nhảy việc, có thể làm thêm bên ngoài (đúng hoặc trái ngành). Nhà cửa/xe cộ: Cái này có thể quá tầm với nhưng cứ viết đây để cố gắng. "},{"id":19,"href":"/blog/posts/openstack-autoscaling-new-approach/","title":"Openstack Autoscaling New Approach","section":"./posts/","content":" NOTE(kiennt): There is a legacy Faythe guideline. The new version is coming soon, check its repository for status.\nThis guide describes how to automatically scale out your Compute instances in response to heavy system usage. By combining with Prometheus pre-defined rules that consider factors such as CPU or memory usage, you can configure OpenStack Orchestration (Heat) to add and remove additional instances automatically, when they are needed.\n1. The standard OpenStack Autoscaling approach # Let\u0026rsquo;s talk about the standard OpenStack Autoscaling approach before goes to the new approach.\n1.1. Main components # Orchestration: The core component providing automatic scaling is Orchestration (heat). Orchestration allows you to define rules using human-readable YAML templates. These rules are applied to evaluate system load based on Telemetry data to find out whether there is need to more instances into the stack. Once the load has dropped, Orchestration can automatically remove the unused instances again.\nTelemetry: Telemetry does performance monitoring of your OpenStack environment, collecting data on CPU, storage and memory utilization for instances and physical hosts. Orchestration templates examine Telemetry data to access whether any pre-defined action should start.\nCeilometer: a data collection service that provides the ability to normalise and transform data across all current OpenStack core components with work underway to support future OpenStack components. Gnocchi: provides a time-series resource indexing, metric storage service with enables users to capture OpenStack resources and the metrics associated with them. Aodh: enables the abiltity to trigger actions based on defined rules against sample or event data collected by Ceilometer. 1.2. Autoscaling process # For more details, you could check IBM help documentation\n1.3. Drawbacks # Ceilometer, Aodh are lacking of contribution. Ceilometer API was deprecated. Either Transform and pipeline was the same state, it means cpu_util will be unusable soon. In the commit message, @sileht - Ceilometer Core reviewer wrote that \u0026ldquo;Also backend like Gnocchi offers a better alternative to compute them\u0026rdquo;. But Aodh still deprecated Gnocchi aggregation API which doesn\u0026rsquo;t support rate:mean. For more details, you can follow the issue I\u0026rsquo;ve opened before. Be honest, I was gave up on it - 3 projects which was tightly related together, one change might cause a sequence and break the whole stack, how can I handle that? Aodh has its own formula to define rule based on Ceilometer metrics (that were stored in Gnocchi). But it isn\u0026rsquo;t correct sometimes cause the wrong scaling action. In reality, I face the case that Rabbitmq was under heavy load due to Ceilometer workload. IMO, Gnocchi documentation is not good enough. It might be a bias personal opinion. 2. The new approach with Faythe # 2.1. The idea # Actually, this isn\u0026rsquo;t a complete new approach, it still leverages Orchestration (heat) to do scaling action. The different comes from Monitor service.\nTake a look at Rico Lin - Heat\u0026rsquo;s PTL, autoscale slide, basically, Autoscaling is the combination of 3 steps:\nMetering. Alarm. Scale. OpenStack Telemetry takes care of Metering and Alarm. Ok, the new approach is simply using another service that can take Telemetry roles.\nThe another service is Prometheus stack. The question here is why I chose this?\nNice query language: Prometheus provides a functional query language called PromQL (Prometheus Query Language) that lets the user select and aggregate time series data in real time. A wide range of exporter: The more exporter the more metrics I can collect and evaluate. Flexibile: Beside the system factor like CPU/Memory usage, I can evaluate any metrics I can collect, for example: JVM metrics. // Take time to investigate about Prometheus and fill it here by yourself 2.2. The implementation # The ideal architecture\n+--------------------------------------------------+ | | | +-----------------+ +-----------------+ | +---------------------+ | | Instance 1 | | Instance 2 | | | | | | | | | | | | Scrape Metrics | +-----------+ | | +-----------+ | | | Prometheus server \u003c------------------------+--------+Exporter(s)| | | |Exporter(s)| | | | | | | +-----------+ | | +-----------+ | | | | | +-----------------+ +-----------------+ | +----------+----------+ | +--------------------------------------+ | | | | Autoscaling Group | | | Fire alerts | +--------------------------------------+ | | | | | | | +----------v------------+ | +--------------------------------------+ | | | Send scale request | | | |Prometheus Alertmanager+----------------------+-----\u003e Scaling Policy | | | | | | | | +-----------------------+ | +--------------------------------------+ | | | | Heat Stack | +--------------------------------------------------+ Prometheus server scrapes metrics from exporters that launch inside Instance. Prometheus server evaluates metrics with pre-defined rules. Prometheus server fires alert to Prometheus alertmanager. Prometheus alertmanager sends POST Scale request to Heat Scaling policy with webhook configuration. It\u0026rsquo;s a piece of cake, right? But where is Faythe, I don\u0026rsquo;t see it? Let\u0026rsquo;s talk about the solution problems:\nPrometheus Alertmanager webhook config doesn\u0026rsquo;t support additional HTTP headers. And they won\u0026rsquo;t! 😢 Heat Scaling Policy signal url requires X-Auth-Token in header and Prometheus can\u0026rsquo;t generate a token itself, either. Heat doesn\u0026rsquo;t recognize the resolved alerts from Prometheus Alertmanager to execute scale in action. How to connect these components together? We need a 3rd service to solve these problems - Faythe does some magic.\nvia GIPHY\nThe reality architecture\n++-------------------------------------------------+ | + | +-----------------+ +-----------------+ | +---------------------+ | | Instance 1 | | Instance 2 | | | | + | | | | | | | Scrape Metrics | +-----------+ | | +-----------+ | | | Prometheus server \u003c------------------------+--------+Exporter(s)| | | |Exporter(s)| | | | | | | +-----------+ | | +-----------+ | | | | | +-----------------+ +-----------------+ | +----------+----------+ | +--------------------------------------+ | | | | Autoscaling Group | | | Fire alerts | +--------------------------------------+ | | | | | | | +----------v------------+ | +--------------------------------------+ | | | | | | | |Prometheus Alertmanager| | Scaling Policy | | | | | | | | +-----------+-----------+ | +-----^--------------------------------+ | | | | | | Send request through | | Heat Stack | | pre-configured webhook +--------------------------------------------------+ | | +-----------v-----------+ | | | | | Faythe +----------------------------------+ | | Send actual scale request +-----------------------+ NOTE: The stack leverages OpenStack instance metadata and Prometheus labels.\nPrometheus server scrapes metrics from exporters that launch inside Instance. Prometheus server evaluates metrics with pre-defined rules. Prometheus server fires alert to Prometheus alertmanager. Prometheus alertmanager sends Alerts via pre-configured webhook URL - Faythe endpoint. Faythe receives and processes Alerts (dedup, group alert and generate a Heat signal URL) and creates a POST request to scale endpoint. 2.3. Guideline # The current aprroach requires some further setup and configuration from Prometheus and Heat stack. You will see that it\u0026rsquo;s quite complicated.\nThe simplify in logic is paid by the complex config steps.\nStep 1: Create a stack - the following is the sample template. It has several requirements:\nOS::Heat::ScalingPolicy has to be named as scaleout_policy and scalein_policy. OS::Heat::AutoScalingGroup\u0026rsquo;s instance metadata has to contain stack_asg_name and stack_asg_id. It will be used to generate signal URL. Instance should have a cloud init script to enable and start Prometheus exporters automatically. --- resources: asg: type: OS::Heat::AutoScalingGroup properties: min_size: { get_param: min_size } max_size: { get_param: max_size } resource: type: { get_param: service_template } properties: flavor: { get_param: flavor } image: { get_param: image } key_name: { get_param: key_name } network: { get_param: network } subnet: { get_param: subnet } metadata: { \u0026#34;monitoring\u0026#34;: \u0026#34;1\u0026#34;, ## Required \u0026#34;service\u0026#34;: \u0026#34;myservice\u0026#34;, \u0026#34;stack_asg_name\u0026#34;: { get_param: \u0026#34;OS::stack_name\u0026#34; }, ## Required \u0026#34;stack_asg_id\u0026#34;: { get_param: \u0026#34;OS::stack_id\u0026#34; }, ## Required } security_group: { get_param: security_group } scaleout_policy: ## Have to be named as `scaleout_policy` type: OS::Heat::ScalingPolicy properties: adjustment_type: change_in_capacity auto_scaling_group_id: { get_resource: asg } cooldown: { get_param: scale_out_cooldown } scaling_adjustment: { get_param: scaling_out_adjustment } scalein_policy: ## Have to be named as `scalein_policy` type: OS::Heat::ScalingPolicy properties: adjustment_type: change_in_capacity auto_scaling_group_id: { get_resource: asg } cooldown: { get_param: scale_in_cooldown } scaling_adjustment: { get_param: scaling_in_adjustment } Step 2: Configure Prometheus openstack discovery\n- job_name: openstack_scale_test openstack_sd_configs: - role: instance identity_endpoint: \u0026#34;\u0026lt;openstackendpoint\u0026gt;\u0026#34; username: \u0026#34;\u0026lt;openstackusername\u0026gt;\u0026#34; password: \u0026#34;\u0026lt;openstackpassword\u0026gt;\u0026#34; domain_name: \u0026#34;default\u0026#34; port: 9100 ## Exporter endpoint refresh_interval: 20s region: \u0026#34;RegionOne\u0026#34; project_name: \u0026#34;\u0026lt;openstackproject\u0026gt;\u0026#34; relabel_configs: ## Only keep metrics from ACTIVE instance - source_labels: [__meta_openstack_instance_status] action: keep regex: ACTIVE ## Only scrape from instance with monitoring tag - source_labels: [__meta_openstack_tag_monitoring] action: keep regex: 1 - source_labels: [__meta_openstack_project_id] target_label: project_id replacement: $1 - source_labels: [__meta_openstack_tag_stack_asg_name] target_label: stack_asg_name replacement: $1 - source_labels: [__meta_openstack_tag_stack_asg_id] target_label: stack_asg_id replacement: $1 Step 3: Define a Prometheus rule, for example:\ngroups: - name: targets rules: - alert: high_memory_load expr: avg by(stack_asg_id, stack_asg_name, project_id) ((node_memory_MemTotal_bytes{service=\u0026#34;myservice\u0026#34;} - node_memory_MemFree_bytes{service=\u0026#34;myservice\u0026#34;}) / node_memory_MemTotal_bytes{service=\u0026#34;myservice\u0026#34;} * 100) \u0026gt; 80 for: 5m labels: severity: critical annotations: summary: \u0026#34;High memory\u0026#34; description: \u0026#34;Instance {{ $labels.instance }} of job {{ $labels.job }} (stack {{ $labels.stack_id }} has been high af for 5m\u0026#34; Step 4: Configure Prometheus Alertmanager webhook, for example:\nroute: receiver: \u0026#34;custom_alert\u0026#34; group_wait: 20s group_interval: 3m receivers: - name: \u0026#34;custom_alert\u0026#34; webhook_configs: - send_resolved: true url: http://\u0026lt;faythe-host\u0026gt;:\u0026lt;faythe-port\u0026gt;/openstack/autoscaling/openstack-1f http_config: basic_auth: username: \u0026#34;admin\u0026#34; password: \u0026#34;password\u0026#34; Note that, openstack-1f has to be the name of OpenStack configuration group in Faythe config file. It helps Faythe to work with multiple OpenStack.\nStep 5: Configure Faythe\n## OpenStackConfiguration. openstack_configs: openstack-1f: region_name: \u0026#34;RegionOne\u0026#34; domain_name: \u0026#34;Default\u0026#34; auth_url: \u0026#34;\u0026lt;openstackendpoint\u0026gt;\u0026#34; username: \u0026#34;\u0026lt;openstackusername\u0026gt;\u0026#34; password: \u0026#34;\u0026lt;openstackpassword\u0026gt;\u0026#34; project_name: \u0026#34;\u0026lt;openstackproject\u0026gt;\u0026#34; server_config: ## Example: ## \u0026#34;www.example.com\u0026#34; ## \u0026#34;([a-z]+).domain.com\u0026#34; ## remote_host_pattern: \u0026#34;10.240.202.209.*\u0026#34; basic_auth: username: \u0026#34;admin\u0026#34; password: \u0026#34;password\u0026#34; log_dir: \u0026#34;/whatever/directory/faythe-logs\u0026#34; Step 6: Let\u0026rsquo;s make them work:\nPrometheus server. Prometheus alertmanager. Faythe. via GIPHY\n2.4. Drawbacks and TODO # Drawbacks\nThe configuration steps is way too complicated, many manual steps have to be done. TODO\nSimplify strategy, might need a service discovery. "},{"id":20,"href":"/blog/gallery/linh-tinh/","title":"Linh Tinh","section":"Gallery","content":"Thỉnh thoảng chụp xong cũng không nhớ chụp hôm nào, chụp về chủ đề gì, tại sao lại chụp. Thôi để tạm đây vậy\u0026hellip;\nDsc03518\nDscf2553\nDscf2555\nDscf2558\nDscf2580\nDscf2589\nDscf2605\nDscf2613\nDscf2637\nDscf2638\nDscf2642\nDscf3001\nDscf3035\nDscf3036\nDscf3037\nDscf3038\nDscf3041\nDscf3045\nDscf3051\nDscf3057\nDscf3061\nDscf3067\nDscf3077\nDscf3078\nDscf3079\nDscf3111\nDscf3112\n"},{"id":21,"href":"/blog/gallery/sapa-2019/","title":"Sapa 2019","section":"Gallery","content":"Have been friends for a long time but there is the first trip together.\nDscf2707\nDscf2716\nDscf2725\nDscf2726\nDscf2738\nDscf2747\nDscf2755\nDscf2758\nDscf2762\nDscf2776\nDscf2777\nDscf2786\nDscf2794\nDscf2795\nDscf2801\nDscf2805\nDscf2829\nDscf2833\nDscf2836\nDscf2842\nDscf2843\nDscf2869\nDscf2870\nDscf2871\n"},{"id":22,"href":"/blog/posts/target-2019/","title":"Mục tiêu 2019","section":"./posts/","content":"Đến hẹn lại lên, thời điểm Tết đến xuân về, tui lại ngồi copy \u0026amp; paste viết mục tiêu phấn đấu cho năm mới. Tui khá là lười và dễ xao nhãng nên việc viết lên các mục tiêu năm mới đơn giản là một cách tự thúc ép bản thân. \u0026ldquo;Mày đã viết ra những điều này, cố mà làm\u0026rdquo;, kiểu vậy. À khoan, hay không viết nữa nhỉ? Lười quá\u0026hellip;\nvia GIPHY\nNah, năm mới rồi nên thay đổi thôi, chững chạc lên nào! Năm nay tui sẽ viết rõ ràng các mục tiêu mang tính định lượng, không viết chung chung như mọi năm nữa. Yo, let\u0026rsquo;s go!\nViết mục tiêu cho 2019: checked hihi. Thu nhập: \u0026lt;salary-2019-per-month\u0026gt; = \u0026lt;salary-2018-per-month\u0026gt; * 1.3. Kỹ năng: Cái này thì nhiều nè, cứ viết ra một số đầu mục, cập nhật sau. Sử dụng thành thạo và ứng dụng Golang vào project thực tết. =1000 commits trên Github.\nHọc về cách thiết kế một hệ thống phân tán giải quyết bài toán thật. Đưa Kubernetes vào ứng dụng thực tế. Củng cố thêm kiến thức đã có hiện tại. Dành thêm thời gian OpenStack (Kolla-Ansible, Zun,\u0026hellip;) \u0026hellip; Sách: 20-30 cuốn sách (sách gì cũng được, truyện trinh thám, văn học kinh điển, tech book). Du lịch: Biên Hòa - Quảng Bình - Hải Phòng - Sơn La. Thể thao: Ngày chống đẩy \u0026gt;=80 (à để mai mùng 1 bắt đầu, nay muộn rồi hihi). Dành 2-3 tối trong tuần để chơi bóng rổ. Blog (là cái này nè): Chăm viết với upload ảnh lên đây hơn. Nhạc nhẽo: Học ít nhất một loại nhạc cụ (ukulele). Chụp ảnh: Dành thêm thời gian cho việc chụp ảnh (Sáng Chủ nhật) để tăng thêm kỹ năng chụp ảnh. Không sợ chụp ở chỗ đông người, tui hay bị ngại\u0026hellip; Còn gì nữa không nhỉ? Có bạn gái!, à tạm thời hết rồi. Hết rồi, bye! Liệu mà làm đấy Kiên!\nP/s: Btw, năm mới mình vẫn sẽ lười thôi, nhưng sẽ lười một cách chân chính hihi!\nvia GIPHY\n"},{"id":23,"href":"/blog/gallery/29-tet-2019/","title":"Xuân 2019: 29 Tết","section":"Gallery","content":"Một chiều lang thang trên phố Tràng Thi.\nDsc03392\nDsc03395\nDsc03396\nDsc03399\nDscf2567\nDscf2570\nDscf2572\nDscf2573\nDscf2574\n"},{"id":24,"href":"/blog/gallery/hanoi-hoankiem-longbien/","title":"Hà Nội: Cầu Long Biên - Hồ Hoàn Kiêm","section":"Gallery","content":"Lang thang quanh hồ Hoàn Kiếm, cầu Long Biên\u0026hellip; Vào một ngày hè, cụ thể ngày nào thì quên mất rồi 🙈\nDscf0511\nDscf0516\nDscf0519\nDscf0520\nDscf0521\nDscf0527\nDscf0538\n"},{"id":25,"href":"/blog/posts/lets-comment/","title":"Lets Comment","section":"./posts/","content":"Hugo ships with support for Disqus, a third-party service that provides comment and community capabilities to websites via JavaScript. But Disqus generates a shit load of page requests and heavy contents\u0026hellip; which even with the benefits that come with having Disqus in place. People just want something that can be used to post a comment, that is.\nThat\u0026rsquo;s why I choose a Disqus alternative - Utterances. Utterances is a lightweight comments widget built on Github issues. Use Github issues for blog comments, wiki pages and more!\nOpen source. 🙌\nNo tracking, no ads, always free. 📡🚫\nNo lock-in. All data stored in GitHub issues. 🔓\nStyled with Primer, the css toolkit that powers GitHub. 💅\nLightweight. Vanilla TypeScript. No font downloads, JavaScript frameworks or polyfills for evergreen browsers. 🐦🌲\nAll these above lines are stolen from Utterances home page!\nSetting up Utterances is quite simple, just follow the home page instruction and you will get what you want.\nSo, let\u0026rsquo;s post some comments here.\n"},{"id":26,"href":"/blog/gallery/","title":"Gallery","section":"About","content":"Stores my photos and pictures that I love!\n"},{"id":27,"href":"/blog/about/","title":"./about/","section":"About","content":" Brief # Engineer Follow @ntk148v Basketball junkie 🏀 Love taking lifestyle photos 📷 \u0026amp; drawing ✏️ OpenSource contributor \u0026amp; lover Getting in touch # Email is probably best, get me directly on kiennt2609@gmail.com or reach me out on my online profiles 👇\n"}]