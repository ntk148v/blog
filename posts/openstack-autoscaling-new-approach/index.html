<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="Hello World, I&#39;m Kien Nguyen. This is my Dark Corner.">
<title>
Openstack Autoscaling New Approach - Ramblings
</title>




<link rel="shortcut icon" href="/blog/kien.ico">








<link rel="stylesheet" href="/blog/css/main.min.121ac3ae920039e7395284aeca218cf12903cae7170a6ab15c8725ae544d3e80.css" integrity="sha256-EhrDrpIAOec5UoSuyiGM8SkDyucXCmqxXIclrlRNPoA=" crossorigin="anonymous" media="screen">



 

<link href="https://fonts.googleapis.com/css?family=Nunito&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Inconsolata&display=swap" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css?family=Patrick+Hand&display=swap" rel="stylesheet">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://ntk148v.github.io/blog/kien.jpg"/>

<meta name="twitter:title" content="Openstack Autoscaling New Approach"/>
<meta name="twitter:description" content="NOTE(kiennt): There is a Faythe guideline
 This guide describes how to automatically scale out your Compute instances in response to heavy system usage. By combining with Prometheus pre-defined rules that consider factors such as CPU or memory usage, you can configure OpenStack Orchestration (Heat) to add and remove additional instances automatically, when they are needed.
1. The standard OpenStack Autoscaling approach Let&rsquo;s talk about the standard OpenStack Autoscaling approach before goes to the new approach."/>

<meta property="og:title" content="Openstack Autoscaling New Approach" />
<meta property="og:description" content="NOTE(kiennt): There is a Faythe guideline
 This guide describes how to automatically scale out your Compute instances in response to heavy system usage. By combining with Prometheus pre-defined rules that consider factors such as CPU or memory usage, you can configure OpenStack Orchestration (Heat) to add and remove additional instances automatically, when they are needed.
1. The standard OpenStack Autoscaling approach Let&rsquo;s talk about the standard OpenStack Autoscaling approach before goes to the new approach." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ntk148v.github.io/blog/posts/openstack-autoscaling-new-approach/" />

<meta property="og:image" content="https://ntk148v.github.io/blog/kien.jpg" />
<meta property="article:published_time" content="2019-08-19T21:19:38+07:00" />
<meta property="article:modified_time" content="2019-08-19T21:19:38+07:00" />



    

    
    
    
    <title>
        
        Openstack Autoscaling New Approach
        
    </title>
</head>

<body>
    
    <div class="wrap">
        
        <div class="section" id="title">Openstack Autoscaling New Approach</div>

        
<div class="section" id="content">
    Mon Aug 19, 2019 &#183; 1538 words
    <div class="authors-container">
        
        
        
        <figcaption><a href="https://ntk148v.github.io/blog/authors/kiennt/">Kien Nguyen-Tuan</a></figcaption>
        
        
        
    </div>
    
    <div class="tag-container">
        
        <span class="tag">
            <a href="https://ntk148v.github.io/blog/tags/blog">
                blog
            </a>
        </span>
        
        <span class="tag">
            <a href="https://ntk148v.github.io/blog/tags/tech">
                tech
            </a>
        </span>
        
    </div>
    
    <hr />
    <aside>
        <nav id="TableOfContents">
<ul>
<li><a href="#1-the-standard-openstack-autoscaling-approach">1. The standard OpenStack Autoscaling approach</a>
<ul>
<li><a href="#1-1-main-components">1.1. Main components</a></li>
<li><a href="#1-2-autoscaling-process">1.2. Autoscaling process</a></li>
<li><a href="#1-3-drawbacks">1.3. Drawbacks</a></li>
</ul></li>
<li><a href="#2-the-new-approach-with-faythe">2. The new approach with Faythe</a>
<ul>
<li><a href="#2-1-the-idea">2.1. The idea</a></li>
<li><a href="#2-2-the-implementation">2.2. The implementation</a></li>
<li><a href="#2-3-guideline">2.3. Guideline</a></li>
<li><a href="#2-4-drawbacks-and-todo">2.4. Drawbacks and TODO</a></li>
</ul></li>
</ul>
</nav>
    </aside>
    

<blockquote>
<p>NOTE(kiennt): There is a <a href="https://github.com/ntk148v/faythe/blob/autoscaling-new-strategy/docs/autoscaling.md">Faythe guideline</a></p>
</blockquote>

<p>This guide describes how to automatically scale out your Compute instances in response to heavy system usage. By combining with Prometheus pre-defined rules that consider factors such as CPU or memory usage, you can configure OpenStack Orchestration (Heat) to add and remove additional instances automatically, when they are needed.</p>

<h1 id="1-the-standard-openstack-autoscaling-approach">1. The standard OpenStack Autoscaling approach</h1>

<p>Let&rsquo;s talk about the standard OpenStack Autoscaling approach before goes to the new approach.</p>

<h2 id="1-1-main-components">1.1. Main components</h2>

<ul>
<li><p>Orchestration: The core component providing automatic scaling is Orchestration (heat). Orchestration allows you to define rules using human-readable YAML templates. These rules are applied to evaluate system load based on Telemetry data to find out whether there is need to more instances into the stack. Once the load has dropped, Orchestration can automatically remove the unused instances again.</p></li>

<li><p>Telemetry: Telemetry does performance monitoring of your OpenStack environment, collecting data on CPU, storage and memory utilization for instances and physical hosts. Orchestration templates examine Telemetry data to access whether any pre-defined action should start.</p>

<ul>
<li>Ceilometer: a data collection service that provides the ability to normalise and transform data across all current OpenStack core components with work underway to support future OpenStack components.</li>
<li>Gnocchi: provides a time-series resource indexing, metric storage service with enables users to capture OpenStack resources and the metrics associated with them.</li>
<li>Aodh: enables the abiltity to trigger actions based on defined rules against sample or event data collected by Ceilometer.</li>
</ul></li>
</ul>

<h2 id="1-2-autoscaling-process">1.2. Autoscaling process</h2>

<p>For more details, you could check <a href="https://ibm-blue-box-help.github.io/help-documentation/heat/autoscaling-with-heat/">IBM help documentation</a></p>

<h2 id="1-3-drawbacks">1.3. Drawbacks</h2>

<ul>
<li>Ceilometer, Aodh are lacking of contribution. Ceilometer API was <a href="https://review.opendev.org/#/c/512286/">deprecated</a>. Either Transform and pipeline was <a href="https://review.opendev.org/#/c/560854/">the same state</a>, it means cpu_util will be unusable soon. In the commit message, @sileht - Ceilometer Core reviewer wrote that &ldquo;Also backend like Gnocchi offers a better alternative to compute them&rdquo;. But Aodh still <a href="https://github.com/openstack/aodh/blob/master/aodh/evaluator/gnocchi.py#L140">deprecated Gnocchi aggregation API</a> which doesn&rsquo;t support <code>rate:mean</code>. For more details, you can follow the <a href="https://github.com/gnocchixyz/gnocchi/issues/999">issue I&rsquo;ve opened before</a>. Be honest, I was gave up on it - 3 projects which was tightly related together, one change might cause a sequence and break the whole stack, how can I handle that?</li>
<li>Aodh has its own formula to define rule based on Ceilometer metrics (that were stored in Gnocchi). But it isn&rsquo;t correct sometimes cause the wrong scaling action.</li>
<li>In reality, I face the case that Rabbitmq was under heavy load due to Ceilometer workload.</li>
<li>IMO, Gnocchi documentation is not good enough. It might be a bias personal opinion.</li>
</ul>

<h1 id="2-the-new-approach-with-faythe">2. The new approach with Faythe</h1>

<h2 id="2-1-the-idea">2.1. The idea</h2>

<p>Actually, this isn&rsquo;t a complete new approach, it still leverages Orchestration (heat) to do scaling action. The different comes from Monitor service.</p>

<p>Take a look at <a href="https://www.slideshare.net/GuanYuLin1/autoscale-a-selfhealing-cluster-in-openstack-with-heat">Rico Lin - Heat&rsquo;s PTL, autoscale slide</a>, basically, Autoscaling is the combination of 3 steps:</p>

<ul>
<li>Metering.</li>
<li>Alarm.</li>
<li>Scale.</li>
</ul>

<p><img src="https://image.slidesharecdn.com/auto-scaleaself-healingclusterinopenstack1-180824033106/95/autoscale-a-selfhealing-cluster-in-openstack-with-heat-21-638.jpg?cb=1536873751" alt="" /></p>

<p>OpenStack Telemetry takes care of <code>Metering</code> and <code>Alarm</code>. Ok, the new approach is simply using <em>another service that can take Telemetry roles</em>.</p>

<p>The <em>another service</em> is <a href="https://prometheus.io/">Prometheus stack</a>. The question here is why I chose this?</p>

<ul>
<li>Nice query language: Prometheus provides a functional query language called PromQL (Prometheus Query Language) that lets the user select and aggregate time series data in real time.</li>
<li>A wide range of exporter: The more exporter the more metrics I can collect and evaluate.</li>
<li>Flexibile: Beside the system factor like CPU/Memory usage, I can evaluate any metrics I can collect, for example: JVM metrics.</li>
<li>// Take time to investigate about Prometheus and fill it here by yourself</li>
</ul>

<h2 id="2-2-the-implementation">2.2. The implementation</h2>

<p><strong>The ideal architecture</strong></p>

<pre><code>                                               +--------------------------------------------------+
                                               |                                                  |
                                               |     +-----------------+  +-----------------+     |
+---------------------+                        |     |   Instance 1    |  |   Instance 2    |     |
|                     |                        |     |                 |  |                 |     |
|                     |            Scrape Metrics    |  +-----------+  |  |  +-----------+  |     |
|  Prometheus server  &lt;------------------------+--------+Exporter(s)|  |  |  |Exporter(s)|  |     |
|                     |                        |     |  +-----------+  |  |  +-----------+  |     |
|                     |                        |     +-----------------+  +-----------------+     |
+----------+----------+                        |     +--------------------------------------+     |
           |                                   |     |           Autoscaling Group          |     |
           | Fire alerts                       |     +--------------------------------------+     |
           |                                   |                                                  |
           |                                   |                                                  |
+----------v------------+                      |     +--------------------------------------+     |
|                       |         Send scale request |                                      |     |
|Prometheus Alertmanager+----------------------+-----&gt;          Scaling Policy              |     |
|                       |                      |     |                                      |     |
+-----------------------+                      |     +--------------------------------------+     |
                                               |                                                  |
                                               |                     Heat Stack                   |
                                               +--------------------------------------------------+
</code></pre>

<ul>
<li>Prometheus server scrapes metrics from exporters that launch inside Instance.</li>
<li>Prometheus server evaluates metrics with pre-defined rules.</li>
<li>Prometheus server fires alert to Prometheus alertmanager.</li>
<li>Prometheus alertmanager sends POST Scale request to Heat Scaling policy with webhook configuration.</li>
</ul>

<p>It&rsquo;s a piece of cake, right? But <em>where is Faythe, I don&rsquo;t see it?</em> Let&rsquo;s talk about the solution problems:</p>

<ul>
<li>Prometheus Alertmanager webhook config doesn&rsquo;t <a href="https://github.com/prometheus/common/issues/140">support additional HTTP headers</a>. And they won&rsquo;t! 😢 Heat Scaling Policy signal url requires <code>X-Auth-Token</code> in header and Prometheus can&rsquo;t generate a token itself, either.</li>
<li>Heat doesn&rsquo;t recognize the resolved alerts from Prometheus Alertmanager to execute scale in action.</li>
<li>How to connect these components together?</li>
</ul>

<p>We need a 3rd service to solve these problems - <code>Faythe does some magic</code>.</p>

<p><center><iframe src="https://giphy.com/embed/12NUbkX6p4xOO4" width="480" height="440" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/shia-labeouf-12NUbkX6p4xOO4">via GIPHY</a></p></center></p>

<p><strong>The reality architecture</strong></p>

<pre><code>                                              ++-------------------------------------------------+
                                               |                                                  +
                                               |     +-----------------+  +-----------------+     |
+---------------------+                        |     |   Instance 1    |  |   Instance 2    |     |
|                     |                        +     |                 |  |                 |     |
|                     |            Scrape Metrics    |  +-----------+  |  |  +-----------+  |     |
|  Prometheus server  &lt;------------------------+--------+Exporter(s)|  |  |  |Exporter(s)|  |     |
|                     |                        |     |  +-----------+  |  |  +-----------+  |     |
|                     |                        |     +-----------------+  +-----------------+     |
+----------+----------+                        |     +--------------------------------------+     |
           |                                   |     |           Autoscaling Group          |     |
           | Fire alerts                       |     +--------------------------------------+     |
           |                                   |                                                  |
           |                                   |                                                  |
+----------v------------+                      |     +--------------------------------------+     |
|                       |                      |     |                                      |     |
|Prometheus Alertmanager|                      |                Scaling Policy              |     |
|                       |                      |     |                                      |     |
+-----------+-----------+                      |     +-----^--------------------------------+     |
            |                                  |           |                                      |
            | Send request through             |           |         Heat Stack                   |
            | pre-configured webhook           +--------------------------------------------------+
            |                                              |
+-----------v-----------+                                  |
|                       |                                  |
|        Faythe         +----------------------------------+
|                       |       Send actual scale request
+-----------------------+

</code></pre>

<blockquote>
<p>NOTE: The stack leverages OpenStack instance metadata and Prometheus labels.</p>
</blockquote>

<ul>
<li>Prometheus server scrapes metrics from exporters that launch inside Instance.</li>
<li>Prometheus server evaluates metrics with pre-defined rules.</li>
<li>Prometheus server fires alert to Prometheus alertmanager.</li>
<li>Prometheus alertmanager sends Alerts via pre-configured webhook URL - Faythe endpoint.</li>
<li>Faythe receives and processes Alerts (dedup, group alert and generate a Heat signal URL) and creates a POST request to scale endpoint.</li>
</ul>

<h2 id="2-3-guideline">2.3. Guideline</h2>

<p>The current aprroach requires some further setup and configuration from Prometheus and Heat stack. You will see that it&rsquo;s quite complicated.</p>

<p><strong>The simplify in logic is paid by the complex config steps.</strong></p>

<p><strong>Step 1:</strong> Create a stack - the following is the sample template. It has several requirements:</p>

<ul>
<li>OS::Heat::ScalingPolicy has to be named as <code>scaleout_policy</code> and <code>scalein_policy</code>.</li>
<li>OS::Heat::AutoScalingGroup&rsquo;s instance metadata has to contain <code>stack_asg_name</code> and <code>stack_asg_id</code>. It will be used to generate signal URL.</li>

<li><p>Instance should have a cloud init script to enable and start Prometheus exporters automatically.</p>
<div class="highlight"><pre style="color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
resources:
asg:
type: OS::Heat::AutoScalingGroup
properties:
  min_size: { get_param: min_size }
  max_size: { get_param: max_size }
  resource:
    type: { get_param: service_template }
    properties:
      flavor: { get_param: flavor }
      image: { get_param: image }
      key_name: { get_param: key_name }
      network: { get_param: network }
      subnet: { get_param: subnet }
      metadata: {
          <span style="color:#00afaf">&#34;monitoring&#34;</span>: <span style="color:#00afaf">&#34;1&#34;</span>, <span style="color:#4e4e4e"># Required</span>
          <span style="color:#00afaf">&#34;service&#34;</span>: <span style="color:#00afaf">&#34;myservice&#34;</span>,
          <span style="color:#00afaf">&#34;stack_asg_name&#34;</span>: { get_param: <span style="color:#00afaf">&#34;OS::stack_name&#34;</span> }, <span style="color:#4e4e4e"># Required</span>
          <span style="color:#00afaf">&#34;stack_asg_id&#34;</span>: { get_param: <span style="color:#00afaf">&#34;OS::stack_id&#34;</span> }, <span style="color:#4e4e4e"># Required</span>
        }
      security_group: { get_param: security_group }

scaleout_policy: <span style="color:#4e4e4e"># Have to be named as `scaleout_policy`</span>
type: OS::Heat::ScalingPolicy
properties:
  adjustment_type: change_in_capacity
  auto_scaling_group_id: { get_resource: asg }
  cooldown: { get_param: scale_out_cooldown }
  scaling_adjustment: { get_param: scaling_out_adjustment }

scalein_policy: <span style="color:#4e4e4e"># Have to be named as `scalein_policy`</span>
type: OS::Heat::ScalingPolicy
properties:
  adjustment_type: change_in_capacity
  auto_scaling_group_id: { get_resource: asg }
  cooldown: { get_param: scale_in_cooldown }
  scaling_adjustment: { get_param: scaling_in_adjustment }</code></pre></div></li>
</ul>

<p><strong>Step 2:</strong> Configure Prometheus openstack discovery</p>
<div class="highlight"><pre style="color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">- job_name: openstack_scale_test
  openstack_sd_configs:
    - role: instance
      identity_endpoint: <span style="color:#00afaf">&#34;&lt;openstackendpoint&gt;&#34;</span>
      username: <span style="color:#00afaf">&#34;&lt;openstackusername&gt;&#34;</span>
      password: <span style="color:#00afaf">&#34;&lt;openstackpassword&gt;&#34;</span>
      domain_name: <span style="color:#00afaf">&#34;default&#34;</span>
      port: <span style="color:#00afaf">9100</span> <span style="color:#4e4e4e"># Exporter endpoint</span>
      refresh_interval: 20s
      region: <span style="color:#00afaf">&#34;RegionOne&#34;</span>
      project_name: <span style="color:#00afaf">&#34;&lt;openstackproject&gt;&#34;</span>

  relabel_configs:
    <span style="color:#4e4e4e"># Only keep metrics from ACTIVE instance</span>
    - source_labels: [__meta_openstack_instance_status]
      action: keep
      regex: ACTIVE

    <span style="color:#4e4e4e"># Only scrape from instance with monitoring tag</span>
    - source_labels: [__meta_openstack_tag_monitoring]
      action: keep
      regex: <span style="color:#00afaf">1</span>

    - source_labels: [__meta_openstack_project_id]
      target_label: project_id
      replacement: $<span style="color:#00afaf">1</span>

    - source_labels: [__meta_openstack_tag_stack_asg_name]
      target_label: stack_asg_name
      replacement: $<span style="color:#00afaf">1</span>

    - source_labels: [__meta_openstack_tag_stack_asg_id]
      target_label: stack_asg_id
      replacement: $<span style="color:#00afaf">1</span></code></pre></div>
<p><strong>Step 3:</strong> Define a Prometheus rule, for example:</p>
<div class="highlight"><pre style="color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">groups:
  - name: targets
    rules:
      - alert: high_memory_load
        expr: avg by(stack_asg_id, stack_asg_name, project_id) ((node_memory_MemTotal_bytes{service=<span style="color:#00afaf">&#34;myservice&#34;</span>} - node_memory_MemFree_bytes{service=<span style="color:#00afaf">&#34;myservice&#34;</span>}) / node_memory_MemTotal_bytes{service=<span style="color:#00afaf">&#34;myservice&#34;</span>} * <span style="color:#00afaf">100</span>) &gt; <span style="color:#00afaf">80</span>
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: <span style="color:#00afaf">&#34;High memory&#34;</span>
          description: <span style="color:#00afaf">&#34;Instance {{ $labels.instance }} of job {{ $labels.job }} (stack {{ $labels.stack_id }} has been high af for 5m&#34;</span></code></pre></div>
<p><strong>Step 4:</strong> Configure Prometheus Alertmanager webhook, for example:</p>
<div class="highlight"><pre style="color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">route:
  receiver: <span style="color:#00afaf">&#34;custom_alert&#34;</span>
  group_wait: 20s
  group_interval: 3m

receivers:
  - name: <span style="color:#00afaf">&#34;custom_alert&#34;</span>
    webhook_configs:
      - send_resolved: <span style="color:#d75f00">true</span>
        url: http://&lt;faythe-host&gt;:&lt;faythe-port&gt;/openstack/autoscaling/openstack-1f
        http_config:
          basic_auth:
            username: <span style="color:#00afaf">&#34;admin&#34;</span>
            password: <span style="color:#00afaf">&#34;password&#34;</span></code></pre></div>
<p>Note that, <code>openstack-1f</code> has to be the name of OpenStack configuration group in Faythe config file. It helps Faythe to work with multiple OpenStack.</p>

<p><strong>Step 5:</strong> Configure Faythe</p>
<div class="highlight"><pre style="color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#4e4e4e"># OpenStackConfiguration.</span>
openstack_configs:
  openstack-1f:
    region_name: <span style="color:#00afaf">&#34;RegionOne&#34;</span>
    domain_name: <span style="color:#00afaf">&#34;Default&#34;</span>
    auth_url: <span style="color:#00afaf">&#34;&lt;openstackendpoint&gt;&#34;</span>
    username: <span style="color:#00afaf">&#34;&lt;openstackusername&gt;&#34;</span>
    password: <span style="color:#00afaf">&#34;&lt;openstackpassword&gt;&#34;</span>
    project_name: <span style="color:#00afaf">&#34;&lt;openstackproject&gt;&#34;</span>

server_config:
  <span style="color:#4e4e4e"># Example:</span>
  <span style="color:#4e4e4e"># &#34;www.example.com&#34;</span>
  <span style="color:#4e4e4e"># &#34;([a-z]+).domain.com&#34;</span>
  <span style="color:#4e4e4e"># remote_host_pattern: &#34;10.240.202.209.*&#34;</span>
  basic_auth:
    username: <span style="color:#00afaf">&#34;admin&#34;</span>
    password: <span style="color:#00afaf">&#34;password&#34;</span>
  log_dir: <span style="color:#00afaf">&#34;/whatever/directory/faythe-logs&#34;</span></code></pre></div>
<p><strong>Step 6:</strong> Let&rsquo;s make them work:</p>

<ul>
<li>Prometheus server.</li>
<li>Prometheus alertmanager.</li>
<li>Faythe.</li>
</ul>

<p><center><iframe src="https://giphy.com/embed/cLlVn5zC5UOSmQZKJ7" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/RobertEBlackmon-bye-go-away-anna-wintour-cLlVn5zC5UOSmQZKJ7">via GIPHY</a></p></center></p>

<h2 id="2-4-drawbacks-and-todo">2.4. Drawbacks and TODO</h2>

<p><strong>Drawbacks</strong></p>

<ul>
<li>The configuration steps is way too complicated, many manual steps have to be done.</li>
</ul>

<p><strong>TODO</strong></p>

<ul>
<li>Simplify strategy, might need a service discovery.</li>
</ul>

</div>


        
<div class="section bottom-menu">
    
<hr />
<script src="https://utteranc.es/client.js" repo="ntk148v/blog" issue-term="pathname" theme="photon-dark" crossorigin="anonymous" async>
</script>
<p>
    
    
    <hr />
    <p>
        

        
        <a href="/blog/posts">back</a>
        
        &#183;
        
        

        
        
        <a href="/blog/posts">
            posts
        </a>
        

        
        
        &#183;
        <a href="/blog/gallery">
            gallery
        </a>
        
        &#183;
        <a href="/blog/authors">
            authors
        </a>
        
        
        &#183;
        <a href="https://ntk148v.github.io/blog">
            main
        </a>
        
    </p>

</div>


        
        <div class="section footer">Copyright © 2018 - Powered by Hugo and Call me Sam theme.
</div>
    </div>
</body>

</html>
