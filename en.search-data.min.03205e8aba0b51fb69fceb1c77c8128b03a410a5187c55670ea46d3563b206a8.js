'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/blog/posts/set-animated-gif-as-wallpaper/','title':"Set Animated Gif as Wallpaper",'section':"Posts",'content':" NOTE: Environment Ubuntu 20.04\n Dependencies #   Xwinwrap:  sudo apt-get install xorg-dev build-essential libx11-dev x11proto-xext-dev libxrender-dev libxext-dev git clone https://github.com/ujjwal96/xwinwrap.git cd xwinwrap make sudo make install make clean  Gifsicle:  sudo apt install gifsicle The helper script #  A helper script to setup animated .gif in dual monitors.\n#!/bin/bash # Uses xwinwrap to display given animated .gif in dual monitors. if [ $# -ne 1 ]; then echo 1\u0026gt;\u0026amp;2 Usage: $0 image.gif exit 1 fi gif=$1 killall -9 xwinwrap killall -9 gifview # Get monitors resolution SCR1=`xrandr | awk \u0026#39;/primary/ \u0026amp;\u0026amp; /connected/ { print $4 }\u0026#39;` SCR2=`xrandr | awk \u0026#39;!/primary/ \u0026amp;\u0026amp; /connected/ { print $3 }\u0026#39;` xwinwrap -g $SCR1 -ov -ni -s -nf -- gifview -w WID $gif -a \u0026amp; xwinwrap -g $SCR2 -ov -ni -s -nf -- gifview -w WID $gif -a \u0026amp; If you want to run xwinwrap by yourself, here is the example:\nxwinwrap -g 1920x1080 -ov -ni -s -nf -- gifview -w WID /full/path/to/gif -a "});index.add({'id':1,'href':'/blog/posts/linux-swap-space-note/','title':"Swap space note",'section':"Posts",'content':"1. What is Swap? #  Swap file systems support virtual memory, data is written to a swap file system when there is not enough RAM to store the data your system is processing.\n2. Swap partition size #  2.1. Old rule of thumb #  swap = 2 * the-amount-of-RAM So if a computer had 64KB of RAM, a swap partition of 128KB would be an optimum size. This rule took into the facts that RAM sizes were typically quite small at the time. Nowadays, RAM has become a cheap \u0026amp; affordable commondity, so the 2x rule is outdated.\n2.2. What is the right amount of swap space? #  Choosing the correct swap size is important. Too much swap space can hide memory leaks, also the storage space is allocated but idle. It can affect the system performance in general.\nFollow the RedHat (CentOS 7x \u0026amp; RHEL 7) guide, the recommended size of a swap partition depending on the amount of RAM \u0026amp; whether you want sufficient memory for your system.\nswap \u0026lt;= 10% * total-size-hard-drives \u0026amp;\u0026amp; swap \u0026lt;= 128GB (if hibernation is allowed)    Amount of RAM Recommended swap space Recommended swap space if allowing for hibernation     \u0026lt; 2GB 2 * the-amount-of-RAM 3 * the-amount-of-RAM   \u0026gt; 2GB - 8GB the-amount-of-RAM 2 * the-amount-of-RAM   \u0026gt; 8GB - 64GB \u0026gt;= 4GB 1.5 * the-amount-of-RAM   \u0026gt; 64GB \u0026gt;= 4GB Hibernation not recommended    3. Common misconceptions \u0026amp; gotchas #  3.1. Increasing swap size would increase performance #   No, it wouldn\u0026rsquo;t. Remember that the slowest part of memory is your hard-disk - swap just provides the ability to use more memory by swapping some pages out to the disk, which is slow compared to RAM operations. Swap can also increase disk I/O \u0026amp; CPU load. This is a tradeoff. Without swap, the OOM may get you. It causes a downtime and in the real life scenario, the application can be slow a bit rather than down completely.  3.2. Swappiness #    The linux kernel tunable parameter vm.swappiness (/proc/sys/vm/swappiness) can be used to define how aggressively memory pages are swapped to disk.\n  The default value: 60. The lower the value, the less swapping is used \u0026amp; the more memory pages are kept in the physical memory.\n* 0: swap is disable. * 1: minimum amount of swapping without disabling it entirely. * 10: recommended value to improve performance when sufficient memory exists in a system * 100: aggressive swapping   Useful commands:\n# Check the current value sysctl vm.swappiness # Adjust the value echo 10 \u0026gt; /proc/sys/vm/swappiness sysctl -w vm.swappiness=10 echo \u0026#34;vm.swappiness = 10\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf   On SSDs, swapping out anonymous pages and reclaiming file pages are essentially equivalent in terms of performance/latency. On older spinning disks, swap reads are slower due to random reads, so a lower vm.swappiness setting makes sense there.\n  3.3. Using swap as emergency memory #   Swap is not generally about getting emergency memory, it\u0026rsquo;s about making memory reclamation egalitarian and efficient. In fact, using it as \u0026ldquo;emergency memory\u0026rdquo; is generally actively harmful.  4. References #    RedHad guideline\n  Chris Down\u0026rsquo;s post\n  Linux Hint - Understanding vm.swappiness\n  "});index.add({'id':2,'href':'/blog/posts/ansitheus/','title':"Ansitheus",'section':"Posts",'content':"Ansitheus = Ansible + Prometheus 1. Prometheus overview #   NOTE: Checkout the Prometheus official documentation.\n Prometheus is an open-source systems monitoring \u0026amp; alerting toolkit originally built at SoundCloud.\n1.1. Features #   a multi-dimensional data model with time series data identified by metric name \u0026amp; key/value pairs PromQL, a flexible query language to leverage this dimensionality no reliance on distributed storage; single server nodes are autonomous time series collection happens via a pull model over HTTP pushing time series is supported via an intermediary gateway targets are discovered via service discovery or static configuration multiple modes of graphing \u0026amp; dashboarding support  1.2. Architecture \u0026amp; components #  Prometheus scrapes metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. It stores all scraped samples locally \u0026amp; runs rules over this data to either aggregate \u0026amp; record new time series from existing data or generate alerts. Grafana or other API consumers can be used to visualize the collected data.\n Prometheus server: scrapes \u0026amp; stores time series data. Prometheus alertmanager: handle alerts. Special-purpose exporters. Push-gateway: support short-lived jobs. client libraries: instrument application code. Various support tools: Grafana,\u0026hellip;  2. Ansitheus #  2.1. Why Ansitheus? #  As you can see that, Prometheus ecosystem consists of multiple components. The operator may need a lot of efforts to configure, deploy \u0026amp; maintain these components. To make life easier, it is necessary to enter the world of automation, using modern tools of configuration management, provisioning \u0026amp; orchestration. Ansible is one of them. It is simple, agentless IT automation that anyone can use. My team decided to choose it as the automation solution, \u0026amp; Ansitheus is the result.\n2.2. Features #  The idea using Ansible to deploy Prometheus is not new. There are many existing solutions:\n cloudalchemy/ansible-prometheus ernestas-poskus/ansible-prometheus \u0026hellip;  So what makes Ansitheus be different with others?\n Deploy, configure \u0026amp; maintain the Prometheus ecosystem easily. Allow users to configure, deploy the system from scratch:  Prepare, configure the local repository. Install the necessary packages. Configure Docker private registry. Configure Docker daemon.   Containerize Prometheus components:  Docker is hotter than hot because it makes it possible to get far more apps running on the same old servers \u0026amp; it also makes it very easy to package \u0026amp; ship programs. You can easily find the advantages of Docker \u0026amp; container through internet.   High availability with HAProxy \u0026amp; Keepalived. Support centralized Docker logging with Fluentd. Support Ansible vault to work with sensitive data.  2.3. Components #   Prometheus Server Prometheus Alertmanager Prometheus Node-exporter Google Cadvisor Prometheus SNMP exporter Haproxy Keepalived Fluentd Grafana Other Prometheus exporters - TODO  2.4. Getting started #    Install Ansible in deployment node.\n  Clone this repostiory.\n  Create configuration directory, default path /etc/ansitheus.\nsudo mkdir -p /etc/ansitheus sudo chown $USER:$USER /etc/ansitheus   Copy config.yml to /etc/ansitheus directory - this is the main configuration for Ansible monitoring tool.\ncp /path/to/ansitheus/repository/etc/ansitheus/config.yml \\  /etc/ansitheus/config.yml   Copy inventory files to the current directory.\ncp /path/to/ansitheus/repository/ansible/inventory/* .   Modify inventory \u0026amp; /etc/ansitheus/config.yml.\n  Run tools/ansitheus, figure out yourself:\nUsage: ./tools/ansitheus COMMAND [option] Options: --inventory, -i \u0026lt;inventory_path\u0026gt; Specify path to ansible inventory file --configdir, -c \u0026lt;config_path\u0026gt; Specify path to directory with config.yml --verbose, -v Increase verbosity of ansible-playbook --tags, -t \u0026lt;tags\u0026gt; Only run plays \u0026amp; tasks tagged with these values --help, -h Show this usage information --skip-common Skip common role --ask-vault-pass Ask for vault password Commands: precheck Do pre-deployment checks for hosts deploy Deploy \u0026amp; start all ansitheus containers pull Pull all images for containers (only pull, no running containers) destroy Destroy Prometheus containers \u0026amp; service configuration --include-images to also destroy Prometheus images --include-volumes to also destroy Prometheus volumes   2.5. Contributors #   Kien Nguyen Dat Vu Duc Nguyen Long Cao  "});index.add({'id':3,'href':'/blog/posts/operate-etcd-cluster/','title':"Operate Etcd cluster",'section':"Posts",'content':" NOTE: This is my perspective aggregation. You can easily find these such of knowledges in the references.\n 1. Context #  Etcd Version v3.4.0.\n2. Requirements #  2.1. Number of nodes #   \u0026gt;= 3 nodes. A etcd cluster needs a majority of nodes, a quorum to agree on updates to the cluster state. For a cluster with n-members, quorum is (n/2)+1.  2.2. CPUs #   Etcd doesn\u0026rsquo;t require a lot of CPU capacity. Typical clusters need 2-4 cores to run smoothly.  2.3. Memory #   Etcd performance depends on having enough memory (cache key-value data, tracking watchers\u0026hellip;). Typical 8GB is enough.  2.4. Disk #   An etcd cluster is very sensitive to disk latencies. Since etcd must persist proposals to its log, disk activity from other processes may cause long fsync latencies. The upshot is etcd may miss heartbeats, causing request timeouts and temporary leader loss. An etcd server can sometimes stably run alongside these processes when given a high disk priority. Check whether a disk is fast enough for etcd using fio. If the 99th percentile of fdatasync is \u0026lt;10ms, your storage is ok.  $ fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data \\  --size=22m --bs=2300 --name=mytest  SSD is recommended.  2.5. Network #   Etcd cluster should be deployed in a fast and reliable network. Low latency ensures etcd members can communicate fast. High bandwidth can reduce the time to recover a failed etcd member. 1GbE is sufficient for common etcd. Note that the network isn\u0026rsquo;t the only source of latency. Each request and response may be impacted by slow disks on both the leader and followers.  3. Tuning #  3.1. Time parameters #   Heartbeat interval.  The frequency with which the leader will notify followers that it is still the leader. Default: 100ms. Best practice: Around 0.5-1.5 x round-trip time (RTT) between members. Measure RTT with ping. Tradeoff: Too low -\u0026gt; etcd will send unnecessary messages -\u0026gt; increase the usage of CPU and network resources. Too high -\u0026gt; leads to high election timeout.   Election timeout.  How long a follower node will go without hearing a heartbeat before attempting to become leader itself. Default: 1000ms. Best practice: \u0026gt;= 10 x RTT and \u0026lt; 50s.   The heartbeat interval and election timeout value should be the same for all members in one cluster.  # Command line arguments: $ etcd --heartbeat-interval=100 --election-timeout=500 # Environment variables: $ ETCD_HEARTBEAT_INTERVAL=100 ETCD_ELECTION_TIMEOUT=500 etcd 3.2. Disk #   An etcd server can sometimes stably run alongside these processes when given a high disk priority using ionice.  # best effort, highest priority $ sudo ionice -c2 -n0 -p `pgrep etcd` 3.3. Snapshot #   etcd appends all key changes to a log file -\u0026gt; huge log that grows forever ‚òùÔ∏è Solution: Make periodic snapshots (save the current and remove old logs). Default: make snapshots after every 10 000 changes. Tuning: Just in case that etcd\u0026rsquo;s memory and disk usage is too high, lower threshold.  # Command line arguments: $ etcd --snapshot-count=5000 # Environment variables: $ ETCD_SNAPSHOT_COUNT=5000 etcd 4. Maintenance #  4.1. History compaction #   Etcd keeps an exact history of its keyspace, the history should be periodically compacted to avoid performance degradation and eventual storage space exhaustion. Etcd can be set to automatically compact the keyspace with the --auto-compaction-* option with a period of hours.  # keep one hour of history $ etcd --auto-compaction-retention=1 --auto-compaction-mode=periodic  Compaction modes:  Revision-based: --auto-compaction-mode=revision --auto-compaction-retention=1000 automatically Compact on \u0026ldquo;latest revision\u0026rdquo; - 1000 every 5-minute (when latest revision is 30000, compact on revision 29000). Use this when having a large keyspace. Periodic: --auto-compaction-mode=periodic --auto-compaction-retention=72h automatically Compact with 72-hour retention window every 1-hour. Use this when having a huge number of revisions for a key-value pair.    4.2. Defragmentation #   Compacting old revisions internally fragments etcd by leaving gaps in backend database - internal fragmentation. Internal fragmentation space is available for use by etcd but unavailable to the host filesystem. Solution: Release this space back to the filesystem with defrag.  $ etcdctl defrag  It should be run rather infrequently, as there is always going to be an unavoidable pause.  5. References #   Etcd hardware: https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md Etcd tuning: https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md Etcd maintainence: https://etcd.io/docs/v3.4.0/op-guide/maintenance/  "});index.add({'id':4,'href':'/blog/posts/golang-block-forever/','title':"Golang: Block forever",'section':"Posts",'content':"Sometimes, you want to block the current goroutine when allowing others to continue. Here is some tricks I\u0026rsquo;ve collected:\n1. References #  Firstly give them some credits:\n https://blog.sgmansfield.com/2016/06/how-to-block-forever-in-go/ https://pliutau.com/different-ways-to-block-go-runtime-forever/   NOTE: I run these with Golang 1.12\n 2. The original #  package main import \u0026#34;fmt\u0026#34; func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(1000) fmt.Println(i) } } func main() { go show() // The main goroutine is exited before the show() be done.  fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 3. Bad - An empty infinite loop #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func forever() { for { // Empty, just do nothing  } } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } An infinite loop here is a busy loop that does nothing except burn CPU time.\n4. Good - Busy blocking #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func forever() { for { runtime.Gosched() } } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } It will reduce your CPU usage but it isn\u0026rsquo;t the preferable solution.\n5. Good - Waiting on itself #  We wait but we never done XD\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func forever() { wg := sync.WaitGroup{} wg.Add(1) wg.Wait() } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 6. Good - Empty select #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func forever() { select{ } } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 7. Good - Double locking #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func forever() { m := sync.Mutex{} // Same with sync.RWMutex  m.Lock() m.Lock() } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 8. Good - Reading an Empty Channel #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func forever() { c := make(chan struct{}) \u0026lt;-c } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } 9. Good - Self produce-and-consume #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func forever() { c := make(chan struct{}, 1) for { select { case \u0026lt;-c: case c \u0026lt;- struct{}{}: } } } func show() { for i := 1; i \u0026lt; 9696969; i++ { time.Sleep(5 * time.Second) fmt.Println(i) } } func main() { go show() forever() fmt.Println(\u0026#34;OK we\u0026#39;re done\u0026#34;) } "});index.add({'id':5,'href':'/blog/posts/rip-kobe-bryant/','title':"Rest In Peace Kobe Bryant",'section':"Posts",'content':"Source: The Undefeated\nI am not Kobe fan honestly but just like him, basketball is something I love. I hope he\u0026rsquo;s at peace because although his journey in life is over, the legacy he left behind is etched in all our souls.\nRest In Peace Mamba üèÄ\n"});index.add({'id':6,'href':'/blog/posts/target-2020/','title':"M·ª•c ti√™u 2020",'section':"Posts",'content':"T·ªïng k·∫øt 2019 #  M·ª•c ti√™u 2019:\n Vi·∫øt m·ª•c ti√™u cho 2019 K·ªπ nƒÉng:  S·ª≠ d·ª•ng th√†nh th·∫°o v√† ·ª©ng d·ª•ng Golang v√†o project th·ª±c t·∫øt. \u0026gt;=1000 commits tr√™n Github. Hey yooooooo! üéâ H·ªçc v·ªÅ c√°ch thi·∫øt k·∫ø m·ªôt h·ªá th·ªëng ph√¢n t√°n gi·∫£i quy·∫øt b√†i to√°n th·∫≠t -\u0026gt; V·∫´n n·ª≠a v·ªùi :( ƒê∆∞a Kubernetes v√†o ·ª©ng d·ª•ng th·ª±c t·∫ø. C·ªßng c·ªë th√™m ki·∫øn th·ª©c ƒë√£ c√≥ hi·ªán t·∫°i, nh∆∞ng ch∆∞a ƒë·ªß üò¢ Th·∫•t b·∫°i ho√†n to√†n: D√†nh th√™m th·ªùi gian OpenStack (Kolla-Ansible, Zun,\u0026hellip;)   S√°ch: 20-30 cu·ªën s√°ch (s√°ch g√¨ c≈©ng ƒë∆∞·ª£c, truy·ªán trinh th√°m, vƒÉn h·ªçc kinh ƒëi·ªÉn, tech book). Du l·ªãch: Qu·∫£ng B√¨nh - H·∫£i Ph√≤ng - Th∆∞·ª£ng H·∫£i\u0026hellip; Th·ªÉ thao: M·ªôt c√¢u chuy·ªán bu·ªìn\u0026hellip; Blog (l√† c√°i n√†y n√®): Bu·ªìn ti·∫øp. Nh·∫°c nh·∫Ωo: Bu·ªìn ti·∫øp^2, m·ªõi t·∫≠p s∆° s∆° ukulele, ch∆∞a ∆∞ng √Ω. Ch·ª•p ·∫£nh: Bu·ªìn ti·∫øp^3.  via GIPHY\n  Qua m·ªôt nƒÉm vi·∫øt ra nh·ªØng m·ª•c ti√™u, m√¨nh nh·∫≠n th·∫•y m·ªôt ƒëi·ªÅu r·∫±ng kh√¥ng ch·ªâ m√¨nh l∆∞·ªùi, m√¨nh c√≤n ch·∫≥ng bi·∫øt th·ª±c s·ª± m√¨nh s·ªëng v√† l√†m vi·ªác ƒë·ªÉ h∆∞·ªõng ƒë·∫øn ƒëi·ªÅu g√¨. V·∫≠t ch·∫•t, hay hi·ªán th·ª±c h√≥a ch√≠nh l√† ti·ªÅn, nh√† c·ª≠a, xe c·ªô, m√¨nh c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c kh√¥ng s·ªõm th√¨ mu·ªôn. Nh∆∞ng sau ƒë√≥ s·∫Ω h∆∞·ªõng ƒë·∫øn g√¨ ti·∫øp theo? via GIPHY\nNh∆∞ng n·∫øu b·∫°n c√≤n kh√¥ng bi·∫øt b·∫£n th√¢n mu·ªën g√¨ v·∫≠y l√†m th·∫ø n√†o ƒë·ªÉ theo ƒëu·ªïi?\n M·ª•c ti√™u 2020 #  via GIPHY\nVi·∫øt t·∫°m ra ƒë√¢y tr∆∞·ªõc, tr∆∞·ªõc ti√™n m√¨nh ph·∫£i t√¨m ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi ph√≠a tr√™n ƒë√£.\n K·ªπ nƒÉng:  Ph√°t huy nh·ªØng g√¨ ƒë√£ l√†m ƒë∆∞·ª£c t·ª´ nƒÉm tr∆∞·ªõc. H·ªçc v·ªÅ k·ªπ nƒÉng cƒÉn b·∫£n t·ªët h∆°n, √≠t nh·∫•t l√† t·ªët cho interview.   Du l·ªãch:  Mi·ªÅn T√¢y Nam B·ªô, yup!!!! L√†o? (Maybe) Phan Rang/L√Ω S∆°n. Hu·∫ø. H·∫£i Ph√≤ng.   S√°ch: ƒê·ªçc th√™m s√°ch Tech. Th·ªÉ thao:  Ti·∫øp t·ª•c c·ªë g·∫Øng ch∆°i b√≥ng r·ªï. Ballin' üèÄ Ti·∫øp t·ª•c t·∫≠p Workout - gi·ªØ v·ªØng nh·ªãp t·∫≠p th·ªÉ d·ª•c bu·ªïi chi·ªÅu.   Ch·ª•p ·∫£nh: C·∫ßn x√°c ƒë·ªãnh r√µ li·ªáu c√≤n y√™u th√≠ch nhi·∫øp ·∫£nh kh√¥ng? Hay ch·ªâ l√† s·ªü th√≠ch nh·∫•t th·ªùi. Ti·∫øng Anh: Ti·∫øng Anh m√¨nh th·∫≠t s·ª± t·ªá, c·∫ßn ph·∫£i ƒëi h·ªçc ti·∫øng Anh nghi√™m t√∫c. √çt nh·∫•t l√† ti·∫øng Anh giao ti·∫øp, tr√°nh vi·ªác n√≥i chuy·ªán b·ªã kh·ªõp. C√¥ng vi·ªác: Suy nghƒ© th·ª±c s·ª± v·ªÅ m·ªôt c∆° h·ªôi ph√°t tri·ªÉn b·∫£n th√¢n kh√°c. C√≥ th·ªÉ nh·∫£y vi·ªác, c√≥ th·ªÉ l√†m th√™m b√™n ngo√†i (ƒë√∫ng ho·∫∑c tr√°i ng√†nh). Nh√† c·ª≠a/xe c·ªô: C√°i n√†y c√≥ th·ªÉ qu√° t·∫ßm v·ªõi nh∆∞ng c·ª© vi·∫øt ƒë√¢y ƒë·ªÉ c·ªë g·∫Øng.  "});index.add({'id':7,'href':'/blog/posts/blog-guideline/','title':"Blog Guideline",'section':"Posts",'content':"In the beginning, I supposed that I\u0026rsquo;m the only one who write-up thing in this blog. But now thing was change, this blog might have multiple bloggers. So it needs a guideline to describe how to contribute.\n1. How to submit a new content #   Here is the source repo. Fork it \u0026amp; start writing. Create a pull request to submit your content. Make sure to create your author page.  2. Create an author page #    Create a directory under content/authors, name it as your desire nickname. For example, your name is amazingblogger.\n  Directory structure.\n  ~/Documents/blog master $? via ‚¨¢ v8.10.0 took 6s tree content/authors content/authors ‚îú‚îÄ‚îÄ donghm ‚îÇ¬†‚îú‚îÄ‚îÄ avatar.jpg ‚îÇ¬†‚îî‚îÄ‚îÄ index.md ‚îú‚îÄ‚îÄ _index.md ‚îî‚îÄ‚îÄ kiennt ‚îú‚îÄ‚îÄ avatar.jpg ‚îî‚îÄ‚îÄ index.md ‚îî‚îÄ‚îÄ amazingblogger ## Here ‚îú‚îÄ‚îÄ avatar.jpg ‚îî‚îÄ‚îÄ index.md  Write about yourself in index.md.  ---name:\u0026#34;Amazing Blogger\u0026#34;contact:twitter:\u0026#34;@blogger\u0026#34;facebook:\u0026#34;blogger\u0026#34;github:\u0026#34;blogger\u0026#34;email:\u0026#34;blogger@gmail.com\u0026#34;website:\u0026#34;https://blogger.io/\u0026#34;---Your amazing personal page here Don\u0026rsquo;t forget to place your avatar in directory. The picture format should be jpg or png, no name restriction.  3. Write a post #   Very similar with author page, just place your post under content/posts.  $ hugo new content/posts/a-new-post.md -t sam  A sample post.  ---title:\u0026#34;Blog Guideline\u0026#34;date:2019-08-22T14:40:59+07:00comments:trueauthors:- kienntshowDate:truetags:[\u0026#34;blog\u0026#34;,\u0026#34;tech\u0026#34;]--- Multiple authors feature is supported. You can disable or enable comment section with comment option. You might want to take look at how I create comment section. Don\u0026rsquo;t forget to add a tag.  4. Create a photo/art gallery #  Hmm, this is my secret corner, so\u0026hellip; Might be in future?\n5. Scripts #  You can notice that some shell scripts are placed in repository. You can only use these if you\u0026rsquo;re the repository collaborator. Just send me a request! üòÑ\n5.1. Lazy pull #  Just a script to init and update submodule, do git pull (both master and gh-pages branchs).\n5.2. Publish to github page #  I deploy the blog from gh-pages branch. You can also tell Github pages to treat your master branch as the published site or point to a separate gh-pages branch. The latter approach is a bit more complex but has some advantages:\n It keeps your source and generated website in different branches and therefore maintains version control history for both. Unlike the preceding docs/ option, it uses the default public folder.  The publish_to_ghpages.sh automates the set up steps.\n"});index.add({'id':8,'href':'/blog/authors/','title':"Authors",'section':"About",'content':"Welcome to the author section!\n"});index.add({'id':9,'href':'/blog/posts/openstack-autoscaling-new-approach/','title':"Openstack Autoscaling New Approach",'section':"Posts",'content':" NOTE(kiennt): There is a legacy Faythe guideline. The new version is coming soon, check its repository for status.\n This guide describes how to automatically scale out your Compute instances in response to heavy system usage. By combining with Prometheus pre-defined rules that consider factors such as CPU or memory usage, you can configure OpenStack Orchestration (Heat) to add and remove additional instances automatically, when they are needed.\n1. The standard OpenStack Autoscaling approach #  Let\u0026rsquo;s talk about the standard OpenStack Autoscaling approach before goes to the new approach.\n1.1. Main components #    Orchestration: The core component providing automatic scaling is Orchestration (heat). Orchestration allows you to define rules using human-readable YAML templates. These rules are applied to evaluate system load based on Telemetry data to find out whether there is need to more instances into the stack. Once the load has dropped, Orchestration can automatically remove the unused instances again.\n  Telemetry: Telemetry does performance monitoring of your OpenStack environment, collecting data on CPU, storage and memory utilization for instances and physical hosts. Orchestration templates examine Telemetry data to access whether any pre-defined action should start.\n Ceilometer: a data collection service that provides the ability to normalise and transform data across all current OpenStack core components with work underway to support future OpenStack components. Gnocchi: provides a time-series resource indexing, metric storage service with enables users to capture OpenStack resources and the metrics associated with them. Aodh: enables the abiltity to trigger actions based on defined rules against sample or event data collected by Ceilometer.    1.2. Autoscaling process #  For more details, you could check IBM help documentation\n1.3. Drawbacks #   Ceilometer, Aodh are lacking of contribution. Ceilometer API was deprecated. Either Transform and pipeline was the same state, it means cpu_util will be unusable soon. In the commit message, @sileht - Ceilometer Core reviewer wrote that \u0026ldquo;Also backend like Gnocchi offers a better alternative to compute them\u0026rdquo;. But Aodh still deprecated Gnocchi aggregation API which doesn\u0026rsquo;t support rate:mean. For more details, you can follow the issue I\u0026rsquo;ve opened before. Be honest, I was gave up on it - 3 projects which was tightly related together, one change might cause a sequence and break the whole stack, how can I handle that? Aodh has its own formula to define rule based on Ceilometer metrics (that were stored in Gnocchi). But it isn\u0026rsquo;t correct sometimes cause the wrong scaling action. In reality, I face the case that Rabbitmq was under heavy load due to Ceilometer workload. IMO, Gnocchi documentation is not good enough. It might be a bias personal opinion.  2. The new approach with Faythe #  2.1. The idea #  Actually, this isn\u0026rsquo;t a complete new approach, it still leverages Orchestration (heat) to do scaling action. The different comes from Monitor service.\nTake a look at Rico Lin - Heat\u0026rsquo;s PTL, autoscale slide, basically, Autoscaling is the combination of 3 steps:\n Metering. Alarm. Scale.  OpenStack Telemetry takes care of Metering and Alarm. Ok, the new approach is simply using another service that can take Telemetry roles.\nThe another service is Prometheus stack. The question here is why I chose this?\n Nice query language: Prometheus provides a functional query language called PromQL (Prometheus Query Language) that lets the user select and aggregate time series data in real time. A wide range of exporter: The more exporter the more metrics I can collect and evaluate. Flexibile: Beside the system factor like CPU/Memory usage, I can evaluate any metrics I can collect, for example: JVM metrics. // Take time to investigate about Prometheus and fill it here by yourself  2.2. The implementation #  The ideal architecture\n +--------------------------------------------------+ | | | +-----------------+ +-----------------+ | +---------------------+ | | Instance 1 | | Instance 2 | | | | | | | | | | | | Scrape Metrics | +-----------+ | | +-----------+ | | | Prometheus server \u0026lt;------------------------+--------+Exporter(s)| | | |Exporter(s)| | | | | | | +-----------+ | | +-----------+ | | | | | +-----------------+ +-----------------+ | +----------+----------+ | +--------------------------------------+ | | | | Autoscaling Group | | | Fire alerts | +--------------------------------------+ | | | | | | | +----------v------------+ | +--------------------------------------+ | | | Send scale request | | | |Prometheus Alertmanager+----------------------+-----\u0026gt; Scaling Policy | | | | | | | | +-----------------------+ | +--------------------------------------+ | | | | Heat Stack | +--------------------------------------------------+  Prometheus server scrapes metrics from exporters that launch inside Instance. Prometheus server evaluates metrics with pre-defined rules. Prometheus server fires alert to Prometheus alertmanager. Prometheus alertmanager sends POST Scale request to Heat Scaling policy with webhook configuration.  It\u0026rsquo;s a piece of cake, right? But where is Faythe, I don\u0026rsquo;t see it? Let\u0026rsquo;s talk about the solution problems:\n Prometheus Alertmanager webhook config doesn\u0026rsquo;t support additional HTTP headers. And they won\u0026rsquo;t! üò¢ Heat Scaling Policy signal url requires X-Auth-Token in header and Prometheus can\u0026rsquo;t generate a token itself, either. Heat doesn\u0026rsquo;t recognize the resolved alerts from Prometheus Alertmanager to execute scale in action. How to connect these components together?  We need a 3rd service to solve these problems - Faythe does some magic.\nvia GIPHY\n The reality architecture\n ++-------------------------------------------------+ | + | +-----------------+ +-----------------+ | +---------------------+ | | Instance 1 | | Instance 2 | | | | + | | | | | | | Scrape Metrics | +-----------+ | | +-----------+ | | | Prometheus server \u0026lt;------------------------+--------+Exporter(s)| | | |Exporter(s)| | | | | | | +-----------+ | | +-----------+ | | | | | +-----------------+ +-----------------+ | +----------+----------+ | +--------------------------------------+ | | | | Autoscaling Group | | | Fire alerts | +--------------------------------------+ | | | | | | | +----------v------------+ | +--------------------------------------+ | | | | | | | |Prometheus Alertmanager| | Scaling Policy | | | | | | | | +-----------+-----------+ | +-----^--------------------------------+ | | | | | | Send request through | | Heat Stack | | pre-configured webhook +--------------------------------------------------+ | | +-----------v-----------+ | | | | | Faythe +----------------------------------+ | | Send actual scale request +-----------------------+  NOTE: The stack leverages OpenStack instance metadata and Prometheus labels.\n  Prometheus server scrapes metrics from exporters that launch inside Instance. Prometheus server evaluates metrics with pre-defined rules. Prometheus server fires alert to Prometheus alertmanager. Prometheus alertmanager sends Alerts via pre-configured webhook URL - Faythe endpoint. Faythe receives and processes Alerts (dedup, group alert and generate a Heat signal URL) and creates a POST request to scale endpoint.  2.3. Guideline #  The current aprroach requires some further setup and configuration from Prometheus and Heat stack. You will see that it\u0026rsquo;s quite complicated.\nThe simplify in logic is paid by the complex config steps.\nStep 1: Create a stack - the following is the sample template. It has several requirements:\n OS::Heat::ScalingPolicy has to be named as scaleout_policy and scalein_policy. OS::Heat::AutoScalingGroup\u0026rsquo;s instance metadata has to contain stack_asg_name and stack_asg_id. It will be used to generate signal URL. Instance should have a cloud init script to enable and start Prometheus exporters automatically.  ---resources:asg:type:OS::Heat::AutoScalingGroupproperties:min_size:{get_param:min_size }max_size:{get_param:max_size }resource:type:{get_param:service_template }properties:flavor:{get_param:flavor }image:{get_param:image }key_name:{get_param:key_name }network:{get_param:network }subnet:{get_param:subnet }metadata:{\u0026#34;monitoring\u0026#34;: \u0026#34;1\u0026#34;,## Required\u0026#34;service\u0026#34;: \u0026#34;myservice\u0026#34;,\u0026#34;stack_asg_name\u0026#34;: {get_param:\u0026#34;OS::stack_name\u0026#34;},## Required\u0026#34;stack_asg_id\u0026#34;: {get_param:\u0026#34;OS::stack_id\u0026#34;},## Required}security_group:{get_param:security_group }scaleout_policy:## Have to be named as `scaleout_policy`type:OS::Heat::ScalingPolicyproperties:adjustment_type:change_in_capacityauto_scaling_group_id:{get_resource:asg }cooldown:{get_param:scale_out_cooldown }scaling_adjustment:{get_param:scaling_out_adjustment }scalein_policy:## Have to be named as `scalein_policy`type:OS::Heat::ScalingPolicyproperties:adjustment_type:change_in_capacityauto_scaling_group_id:{get_resource:asg }cooldown:{get_param:scale_in_cooldown }scaling_adjustment:{get_param:scaling_in_adjustment }Step 2: Configure Prometheus openstack discovery\n- job_name:openstack_scale_testopenstack_sd_configs:- role:instanceidentity_endpoint:\u0026#34;\u0026lt;openstackendpoint\u0026gt;\u0026#34;username:\u0026#34;\u0026lt;openstackusername\u0026gt;\u0026#34;password:\u0026#34;\u0026lt;openstackpassword\u0026gt;\u0026#34;domain_name:\u0026#34;default\u0026#34;port:9100## Exporter endpointrefresh_interval:20sregion:\u0026#34;RegionOne\u0026#34;project_name:\u0026#34;\u0026lt;openstackproject\u0026gt;\u0026#34;relabel_configs:## Only keep metrics from ACTIVE instance- source_labels:[__meta_openstack_instance_status]action:keepregex:ACTIVE## Only scrape from instance with monitoring tag- source_labels:[__meta_openstack_tag_monitoring]action:keepregex:1- source_labels:[__meta_openstack_project_id]target_label:project_idreplacement:$1- source_labels:[__meta_openstack_tag_stack_asg_name]target_label:stack_asg_namereplacement:$1- source_labels:[__meta_openstack_tag_stack_asg_id]target_label:stack_asg_idreplacement:$1Step 3: Define a Prometheus rule, for example:\ngroups:- name:targetsrules:- alert:high_memory_loadexpr:avg by(stack_asg_id, stack_asg_name, project_id) ((node_memory_MemTotal_bytes{service=\u0026#34;myservice\u0026#34;} - node_memory_MemFree_bytes{service=\u0026#34;myservice\u0026#34;}) / node_memory_MemTotal_bytes{service=\u0026#34;myservice\u0026#34;} * 100) \u0026gt; 80for:5mlabels:severity:criticalannotations:summary:\u0026#34;High memory\u0026#34;description:\u0026#34;Instance {{ $labels.instance }} of job {{ $labels.job }} (stack {{ $labels.stack_id }} has been high af for 5m\u0026#34;Step 4: Configure Prometheus Alertmanager webhook, for example:\nroute:receiver:\u0026#34;custom_alert\u0026#34;group_wait:20sgroup_interval:3mreceivers:- name:\u0026#34;custom_alert\u0026#34;webhook_configs:- send_resolved:trueurl:http://\u0026lt;faythe-host\u0026gt;:\u0026lt;faythe-port\u0026gt;/openstack/autoscaling/openstack-1fhttp_config:basic_auth:username:\u0026#34;admin\u0026#34;password:\u0026#34;password\u0026#34;Note that, openstack-1f has to be the name of OpenStack configuration group in Faythe config file. It helps Faythe to work with multiple OpenStack.\nStep 5: Configure Faythe\n## OpenStackConfiguration.openstack_configs:openstack-1f:region_name:\u0026#34;RegionOne\u0026#34;domain_name:\u0026#34;Default\u0026#34;auth_url:\u0026#34;\u0026lt;openstackendpoint\u0026gt;\u0026#34;username:\u0026#34;\u0026lt;openstackusername\u0026gt;\u0026#34;password:\u0026#34;\u0026lt;openstackpassword\u0026gt;\u0026#34;project_name:\u0026#34;\u0026lt;openstackproject\u0026gt;\u0026#34;server_config:## Example:## \u0026#34;www.example.com\u0026#34;## \u0026#34;([a-z]+).domain.com\u0026#34;## remote_host_pattern: \u0026#34;10.240.202.209.*\u0026#34;basic_auth:username:\u0026#34;admin\u0026#34;password:\u0026#34;password\u0026#34;log_dir:\u0026#34;/whatever/directory/faythe-logs\u0026#34;Step 6: Let\u0026rsquo;s make them work:\n Prometheus server. Prometheus alertmanager. Faythe.  via GIPHY\n 2.4. Drawbacks and TODO #  Drawbacks\n The configuration steps is way too complicated, many manual steps have to be done.  TODO\n Simplify strategy, might need a service discovery.  "});index.add({'id':10,'href':'/blog/gallery/linh-tinh/','title':"Linh Tinh",'section':"Gallery",'content':"Th·ªânh tho·∫£ng ch·ª•p xong c≈©ng kh√¥ng nh·ªõ ch·ª•p h√¥m n√†o, ch·ª•p v·ªÅ ch·ªß ƒë·ªÅ g√¨, t·∫°i sao l·∫°i ch·ª•p. Th√¥i ƒë·ªÉ t·∫°m ƒë√¢y v·∫≠y\u0026hellip;\n"});index.add({'id':11,'href':'/blog/gallery/sapa-2019/','title':"Sapa 2019",'section':"Gallery",'content':"Have been friends for a long time but there is the first trip together.\n"});index.add({'id':12,'href':'/blog/posts/target-2019/','title':"M·ª•c ti√™u 2019",'section':"Posts",'content':"ƒê·∫øn h·∫πn l·∫°i l√™n, th·ªùi ƒëi·ªÉm T·∫øt ƒë·∫øn xu√¢n v·ªÅ, tui l·∫°i ng·ªìi copy \u0026amp; paste vi·∫øt m·ª•c ti√™u ph·∫•n ƒë·∫•u cho nƒÉm m·ªõi. Tui kh√° l√† l∆∞·ªùi v√† d·ªÖ xao nh√£ng n√™n vi·ªác vi·∫øt l√™n c√°c m·ª•c ti√™u nƒÉm m·ªõi ƒë∆°n gi·∫£n l√† m·ªôt c√°ch t·ª± th√∫c √©p b·∫£n th√¢n. \u0026ldquo;M√†y ƒë√£ vi·∫øt ra nh·ªØng ƒëi·ªÅu n√†y, c·ªë m√† l√†m\u0026rdquo;, ki·ªÉu v·∫≠y. √Ä khoan, hay kh√¥ng vi·∫øt n·ªØa nh·ªâ? L∆∞·ªùi qu√°\u0026hellip;\nvia GIPHY\nNah, nƒÉm m·ªõi r·ªìi n√™n thay ƒë·ªïi th√¥i, ch·ªØng ch·∫°c l√™n n√†o! NƒÉm nay tui s·∫Ω vi·∫øt r√µ r√†ng c√°c m·ª•c ti√™u mang t√≠nh ƒë·ªãnh l∆∞·ª£ng, kh√¥ng vi·∫øt chung chung nh∆∞ m·ªçi nƒÉm n·ªØa. Yo, let\u0026rsquo;s go!\n Vi·∫øt m·ª•c ti√™u cho 2019: checked hihi. Thu nh·∫≠p:  \u0026lt;salary-2019-per-month\u0026gt; = \u0026lt;salary-2018-per-month\u0026gt; * 1.3.  K·ªπ nƒÉng: C√°i n√†y th√¨ nhi·ªÅu n√®, c·ª© vi·∫øt ra m·ªôt s·ªë ƒë·∫ßu m·ª•c, c·∫≠p nh·∫≠t sau.  S·ª≠ d·ª•ng th√†nh th·∫°o v√† ·ª©ng d·ª•ng Golang v√†o project th·ª±c t·∫øt.   =1000 commits tr√™n Github.\n  H·ªçc v·ªÅ c√°ch thi·∫øt k·∫ø m·ªôt h·ªá th·ªëng ph√¢n t√°n gi·∫£i quy·∫øt b√†i to√°n th·∫≠t. ƒê∆∞a Kubernetes v√†o ·ª©ng d·ª•ng th·ª±c t·∫ø. C·ªßng c·ªë th√™m ki·∫øn th·ª©c ƒë√£ c√≥ hi·ªán t·∫°i. D√†nh th√™m th·ªùi gian OpenStack (Kolla-Ansible, Zun,\u0026hellip;) \u0026hellip;   S√°ch: 20-30 cu·ªën s√°ch (s√°ch g√¨ c≈©ng ƒë∆∞·ª£c, truy·ªán trinh th√°m, vƒÉn h·ªçc kinh ƒëi·ªÉn, tech book). Du l·ªãch: Bi√™n H√≤a - Qu·∫£ng B√¨nh - H·∫£i Ph√≤ng - S∆°n La. Th·ªÉ thao:  Ng√†y ch·ªëng ƒë·∫©y \u0026gt;=80 (√† ƒë·ªÉ mai m√πng 1 b·∫Øt ƒë·∫ßu, nay mu·ªôn r·ªìi hihi). D√†nh 2-3 t·ªëi trong tu·∫ßn ƒë·ªÉ ch∆°i b√≥ng r·ªï.   Blog (l√† c√°i n√†y n√®): ChƒÉm vi·∫øt v·ªõi upload ·∫£nh l√™n ƒë√¢y h∆°n. Nh·∫°c nh·∫Ωo: H·ªçc √≠t nh·∫•t m·ªôt lo·∫°i nh·∫°c c·ª• (ukulele). Ch·ª•p ·∫£nh: D√†nh th√™m th·ªùi gian cho vi·ªác ch·ª•p ·∫£nh (S√°ng Ch·ªß nh·∫≠t) ƒë·ªÉ tƒÉng th√™m k·ªπ nƒÉng ch·ª•p ·∫£nh. Kh√¥ng s·ª£ ch·ª•p ·ªü ch·ªó ƒë√¥ng ng∆∞·ªùi, tui hay b·ªã ng·∫°i\u0026hellip; C√≤n g√¨ n·ªØa kh√¥ng nh·ªâ? C√≥ b·∫°n g√°i!, √† t·∫°m th·ªùi h·∫øt r·ªìi.  H·∫øt r·ªìi, bye! Li·ªáu m√† l√†m ƒë·∫•y Ki√™n!\nP/s: Btw, nƒÉm m·ªõi m√¨nh v·∫´n s·∫Ω l∆∞·ªùi th√¥i, nh∆∞ng s·∫Ω l∆∞·ªùi m·ªôt c√°ch ch√¢n ch√≠nh hihi!\nvia GIPHY\n"});index.add({'id':13,'href':'/blog/gallery/xuan-2019/','title':"Xu√¢n 2019: 29 T·∫øt",'section':"Gallery",'content':"M·ªôt chi·ªÅu lang thang tr√™n ph·ªë Tr√†ng Thi.\n"});index.add({'id':14,'href':'/blog/gallery/hanoi-longbien-hoankiem/','title':"H√† N·ªôi: C·∫ßu Long Bi√™n - H·ªì Ho√†n Ki√™m",'section':"Gallery",'content':"Lang thang quanh h·ªì Ho√†n Ki·∫øm, c·∫ßu Long Bi√™n\u0026hellip; V√†o m·ªôt ng√†y h√®, c·ª• th·ªÉ ng√†y n√†o th√¨ qu√™n m·∫•t r·ªìi üôà\n"});index.add({'id':15,'href':'/blog/posts/lets-comment/','title':"Lets Comment",'section':"Posts",'content':"Hugo ships with support for Disqus, a third-party service that provides comment and community capabilities to websites via JavaScript. But Disqus generates a shit load of page requests and heavy contents\u0026hellip; which even with the benefits that come with having Disqus in place. People just want something that can be used to post a comment, that is.\nThat\u0026rsquo;s why I choose a Disqus alternative - Utterances. Utterances is a lightweight comments widget built on Github issues. Use Github issues for blog comments, wiki pages and more!\n  Open source. üôå\n  No tracking, no ads, always free. üì°üö´\n  No lock-in. All data stored in GitHub issues. üîì\n  Styled with Primer, the css toolkit that powers GitHub. üíÖ\n  Lightweight. Vanilla TypeScript. No font downloads, JavaScript frameworks or polyfills for evergreen browsers. üê¶üå≤\n   All these above lines are stolen from Utterances home page!\n Setting up Utterances is quite simple, just follow the home page instruction and you will get what you want.\nSo, let\u0026rsquo;s post some comments here.\n"});index.add({'id':16,'href':'/blog/gallery/','title':"Gallery",'section':"About",'content':"Stores my photos and pictures that I love!\n"});index.add({'id':17,'href':'/blog/posts/','title':"Posts",'section':"About",'content':"Welcome to the blog section!\n"});index.add({'id':18,'href':'/blog/authors/donghm/','title':"Index",'section':"Authors",'content':"ƒê√¥ng Hi·∫øp secret page.\n"});index.add({'id':19,'href':'/blog/authors/kiennt/','title':"Index",'section':"Authors",'content':"Brief #   Devops Golang developer Python developer üêç. Basketball junkie üèÄ. Love taking lifestyle photos üì∑ \u0026amp; drawing ‚úèÔ∏è. OpenStack contributor \u0026amp; lover. Linux lover üòò \u0026amp; Window hater üôÖ‚Äç‚ôÄÔ∏è.   A computer is like air conditioning - it becomes useless when you open Windows. -- Linus Torvalds \u0026ndash;\n Getting in touch #  Email is probably best, get me directly on kiennt2609@gmail.com or reach me out on my online profiles üëá\n                       "});})();